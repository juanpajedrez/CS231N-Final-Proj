{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Pre trained to test\n",
    "\n",
    "This jupyter notebook has the objective to, not only retrieve the accuracies of the VGGnet16 pretrained, but to obtain also <br>\n",
    "the layer features before the last classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/cs231/CS231N-Final-Proj/notebooks\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "notebooks_path = os.getcwd() # OR MAYBE has to be set manually depending your computer\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(notebooks_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "df_path = os.path.join(notebooks_path, os.pardir, \"data\")\n",
    "data_path = os.path.join(df_path, \"images\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00, 19.26it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 20.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(df_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the images and labels\n",
    "\n",
    "Also, obtain DataLoaders for test, train, and validation datasets using <br>\n",
    "the Dataloader class from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([14])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 14])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 256x256\n",
    "    #transforms.CenterCrop((224, 224)),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Obtain the random sampler of the dataset\n",
    "test_sampler = RandomSampler(test_dataset)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 32, and shuffle true, and num workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vgg16 pretrained model\n",
    "\n",
    "Check if you have GPU Envidia! Else, use the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Load the pretrained model\n",
    "vgg16 = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the classifier layers\n",
    "We know that VGGnet16 has a last linear layer with 1000 output units...<br>\n",
    "However, this doesnt really resemble our problem per se...<br><br>\n",
    "Lets do this! Lets replace the last layer with a linear layer that has the same <br> number of classes as our data!. (In our case, is 20).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.features architecture and get the parameter shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64]), torch.Size([64, 64, 3, 3]), torch.Size([64]), torch.Size([128, 64, 3, 3]), torch.Size([128]), torch.Size([128, 128, 3, 3]), torch.Size([128]), torch.Size([256, 128, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([512, 256, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.features)\n",
    "print([x.shape for x in vgg16.features.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.avgpool parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.avgpool)\n",
    "print([x.shape for x in vgg16.avgpool.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16 classifier parameters and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "[torch.Size([4096, 25088]), torch.Size([4096]), torch.Size([4096, 4096]), torch.Size([4096]), torch.Size([1000, 4096]), torch.Size([1000])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.classifier)\n",
    "print([x.shape for x in vgg16.classifier.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL to conduct fine-tuning on Vggnet16 only on the last (Linear) layer\n",
    "\n",
    "# First, freeze all the parameters\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=20, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the last layer for the last 20 classes\n",
    "num_classes = 20  # Number of classes for your specific task\n",
    "num_features = vgg16.classifier[0].in_features #Get all of the features after convolutional layers\n",
    "print(num_features)\n",
    "\n",
    "#Obtain the same classifier you got befor with lower number of classes, so we can pretrain it\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, num_classes, bias=True),\n",
    "    nn.ReLU(inplace= True),\n",
    ")\n",
    "\n",
    "print(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "# Create state_dict path\n",
    "model_dict_path = os.path.join(notebooks_path, os.pardir, \"models\")\n",
    "\n",
    "if os.path.exists(model_dict_path) == False:\n",
    "    os.mkdir(model_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object Module.parameters at 0x7170581f9b60>]\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL to perform fine-tuning\n",
    "#print([x.shape for x in vgg16.classifier[-6].parameters()])\n",
    "\n",
    "#Set the learning rate to be 1e-3 default\n",
    "lr_set = 1e-3\n",
    "\n",
    "import torch.optim as optim\n",
    "vgg16 = vgg16.to(device)\n",
    "vgg16.train()\n",
    "params_to_update = [vgg16.classifier.parameters()]\n",
    "print(params_to_update)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(*params_to_update, lr=lr_set)\n",
    "\n",
    "def finetune_model(model, data_loader, num_epochs, device:str):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-------------')\n",
    "            \n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            # Convert labels to float type (also need to move to CUDA again!)\n",
    "            images_labels = images_labels.to(torch.float64)\n",
    "\n",
    "            # initialize optimizer\n",
    "            optimizer.zero_grad()            \n",
    "            outputs = model(images_inputs)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(outputs, images_labels)\n",
    "            \n",
    "            # predict labels\n",
    "            pred_labels = (outputs > 0.5).float()\n",
    "\n",
    "            # Calculate TP, FP, TN, FN and accuracy\n",
    "            TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0                \n",
    "\n",
    "            loss.backward()\n",
    "            print(f\"iter {idx} ---  Loss: {loss}    Accuracy: {accuracy}\")\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save parameters for each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_dict_path, \"vgg16_finetune_params.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 ---  Loss: 0.795275045933522    Accuracy: 78.59375\n",
      "iter 1 ---  Loss: 0.7853673950714438    Accuracy: 80.0\n",
      "iter 2 ---  Loss: 0.736755620627082    Accuracy: 88.125\n",
      "iter 3 ---  Loss: 0.7368404298822497    Accuracy: 86.5625\n",
      "iter 4 ---  Loss: 0.7246010590242804    Accuracy: 90.0\n",
      "iter 5 ---  Loss: 0.723800438617036    Accuracy: 89.0625\n",
      "iter 6 ---  Loss: 0.7140224423630571    Accuracy: 91.40625\n",
      "iter 7 ---  Loss: 0.7142588473958313    Accuracy: 91.5625\n",
      "iter 8 ---  Loss: 0.7136291054321191    Accuracy: 92.34375\n",
      "iter 9 ---  Loss: 0.7059939804734313    Accuracy: 90.9375\n",
      "iter 10 ---  Loss: 0.7032635762767314    Accuracy: 92.65625\n",
      "iter 11 ---  Loss: 0.7017985391954427    Accuracy: 92.96875\n",
      "iter 12 ---  Loss: 0.7027069256451797    Accuracy: 91.09375\n",
      "iter 13 ---  Loss: 0.7065384114990594    Accuracy: 92.1875\n",
      "iter 14 ---  Loss: 0.702552700326487    Accuracy: 91.25\n",
      "iter 15 ---  Loss: 0.6996037155608065    Accuracy: 93.28125\n",
      "iter 16 ---  Loss: 0.7008088722421235    Accuracy: 92.65625\n",
      "iter 17 ---  Loss: 0.7003905230681995    Accuracy: 91.5625\n",
      "iter 18 ---  Loss: 0.7004074908541953    Accuracy: 92.5\n",
      "iter 19 ---  Loss: 0.6985161309232354    Accuracy: 92.1875\n",
      "iter 20 ---  Loss: 0.6974491576402215    Accuracy: 91.25\n",
      "iter 21 ---  Loss: 0.7022661148432235    Accuracy: 93.125\n",
      "iter 22 ---  Loss: 0.6992143476745696    Accuracy: 91.71875\n",
      "iter 23 ---  Loss: 0.6994294097785314    Accuracy: 93.90625\n",
      "iter 24 ---  Loss: 0.6985977960692253    Accuracy: 92.65625\n",
      "iter 25 ---  Loss: 0.6967929896367423    Accuracy: 93.125\n",
      "iter 26 ---  Loss: 0.6965238633849368    Accuracy: 92.03125\n",
      "iter 27 ---  Loss: 0.6958596686119564    Accuracy: 91.40625\n",
      "iter 28 ---  Loss: 0.6984159456274939    Accuracy: 92.1875\n",
      "iter 29 ---  Loss: 0.6962835400336189    Accuracy: 91.25\n",
      "iter 30 ---  Loss: 0.6943711764801265    Accuracy: 92.96875\n",
      "iter 31 ---  Loss: 0.69530048481829    Accuracy: 92.1875\n",
      "iter 32 ---  Loss: 0.6978200060693326    Accuracy: 91.875\n",
      "iter 33 ---  Loss: 0.6953058388222417    Accuracy: 92.5\n",
      "iter 34 ---  Loss: 0.6972200299405813    Accuracy: 91.09375\n",
      "iter 35 ---  Loss: 0.6954581312522351    Accuracy: 92.8125\n",
      "iter 36 ---  Loss: 0.6956800771979034    Accuracy: 92.03125\n",
      "iter 37 ---  Loss: 0.6971048185580003    Accuracy: 93.90625\n",
      "iter 38 ---  Loss: 0.6964584190776805    Accuracy: 91.25\n",
      "iter 39 ---  Loss: 0.6962906650966034    Accuracy: 92.34375\n",
      "iter 40 ---  Loss: 0.6949368345987751    Accuracy: 93.59375\n",
      "iter 41 ---  Loss: 0.6952303587997449    Accuracy: 91.25\n",
      "iter 42 ---  Loss: 0.6939328723528888    Accuracy: 92.1875\n",
      "iter 43 ---  Loss: 0.6958866154160206    Accuracy: 92.5\n",
      "iter 44 ---  Loss: 0.6945292769240041    Accuracy: 93.28125\n",
      "iter 45 ---  Loss: 0.696575210741139    Accuracy: 91.71875\n",
      "iter 46 ---  Loss: 0.695771620443702    Accuracy: 91.25\n",
      "iter 47 ---  Loss: 0.6951571657482418    Accuracy: 92.1875\n",
      "iter 48 ---  Loss: 0.6946660705056275    Accuracy: 92.03125\n",
      "iter 49 ---  Loss: 0.6939680045066781    Accuracy: 91.71875\n",
      "iter 50 ---  Loss: 0.6947860486267019    Accuracy: 92.34375\n",
      "iter 51 ---  Loss: 0.6943608444664279    Accuracy: 94.21875\n",
      "iter 52 ---  Loss: 0.6938328060612549    Accuracy: 92.8125\n",
      "iter 53 ---  Loss: 0.6942664741553017    Accuracy: 91.5625\n",
      "iter 54 ---  Loss: 0.6941880927828606    Accuracy: 92.96875\n",
      "iter 55 ---  Loss: 0.6935257000033744    Accuracy: 93.125\n",
      "iter 56 ---  Loss: 0.6947867549111835    Accuracy: 93.28125\n",
      "iter 57 ---  Loss: 0.693685197121522    Accuracy: 91.875\n",
      "iter 58 ---  Loss: 0.6952514993759906    Accuracy: 92.8125\n",
      "iter 59 ---  Loss: 0.6947306134818064    Accuracy: 92.03125\n",
      "iter 60 ---  Loss: 0.693802935686108    Accuracy: 93.75\n",
      "iter 61 ---  Loss: 0.6940524333680514    Accuracy: 91.5625\n",
      "iter 62 ---  Loss: 0.6946096287090768    Accuracy: 92.65625\n",
      "iter 63 ---  Loss: 0.6944714290439151    Accuracy: 92.8125\n",
      "iter 64 ---  Loss: 0.693811884985189    Accuracy: 92.34375\n",
      "iter 65 ---  Loss: 0.694905831469805    Accuracy: 91.875\n",
      "iter 66 ---  Loss: 0.693603390143835    Accuracy: 91.71875\n",
      "iter 67 ---  Loss: 0.6942115902078513    Accuracy: 91.5625\n",
      "iter 68 ---  Loss: 0.6939718771493063    Accuracy: 91.875\n",
      "iter 69 ---  Loss: 0.6943351199523022    Accuracy: 90.9375\n",
      "iter 70 ---  Loss: 0.6941958338640689    Accuracy: 92.8125\n",
      "iter 71 ---  Loss: 0.6952273995262659    Accuracy: 92.1875\n",
      "iter 72 ---  Loss: 0.6954297093063361    Accuracy: 92.8125\n",
      "iter 73 ---  Loss: 0.6949567828909494    Accuracy: 92.96875\n",
      "iter 74 ---  Loss: 0.6946329220256303    Accuracy: 92.03125\n",
      "iter 75 ---  Loss: 0.6944509921188001    Accuracy: 91.40625\n",
      "iter 76 ---  Loss: 0.6954584475955926    Accuracy: 92.8125\n",
      "iter 77 ---  Loss: 0.6939241327840137    Accuracy: 92.8125\n",
      "iter 78 ---  Loss: 0.6942040519119473    Accuracy: 93.90625\n",
      "iter 79 ---  Loss: 0.6940466919582833    Accuracy: 93.125\n",
      "iter 80 ---  Loss: 0.6936155093564594    Accuracy: 93.28125\n",
      "iter 81 ---  Loss: 0.6931935871456517    Accuracy: 92.5\n",
      "iter 82 ---  Loss: 0.6937743648886681    Accuracy: 92.65625\n",
      "iter 83 ---  Loss: 0.6941302828592598    Accuracy: 91.25\n",
      "iter 84 ---  Loss: 0.6933010048524011    Accuracy: 91.71875\n",
      "iter 85 ---  Loss: 0.6939359424286522    Accuracy: 92.96875\n",
      "iter 86 ---  Loss: 0.6939392559725093    Accuracy: 92.5\n",
      "iter 87 ---  Loss: 0.6945666261599399    Accuracy: 92.65625\n",
      "iter 88 ---  Loss: 0.6937028954096605    Accuracy: 92.8125\n",
      "iter 89 ---  Loss: 0.693899080096162    Accuracy: 92.1875\n",
      "iter 90 ---  Loss: 0.6944480315432884    Accuracy: 92.65625\n",
      "iter 91 ---  Loss: 0.6934041541069746    Accuracy: 93.4375\n",
      "iter 92 ---  Loss: 0.6936420793208526    Accuracy: 91.875\n",
      "iter 93 ---  Loss: 0.6933514317614027    Accuracy: 92.65625\n",
      "iter 94 ---  Loss: 0.6940419135826232    Accuracy: 92.65625\n",
      "iter 95 ---  Loss: 0.6936957161364262    Accuracy: 91.875\n",
      "iter 96 ---  Loss: 0.6937075326626655    Accuracy: 91.5625\n",
      "iter 97 ---  Loss: 0.6938876471715049    Accuracy: 92.34375\n",
      "iter 98 ---  Loss: 0.6940468611210235    Accuracy: 91.875\n",
      "iter 99 ---  Loss: 0.694079244308523    Accuracy: 92.8125\n",
      "iter 100 ---  Loss: 0.6943184002011549    Accuracy: 92.65625\n",
      "iter 101 ---  Loss: 0.694462826907693    Accuracy: 92.34375\n",
      "iter 102 ---  Loss: 0.6936861917260103    Accuracy: 93.28125\n",
      "iter 103 ---  Loss: 0.6943106422389974    Accuracy: 93.75\n",
      "iter 104 ---  Loss: 0.6936881760942925    Accuracy: 92.1875\n",
      "iter 105 ---  Loss: 0.6928076066076756    Accuracy: 92.5\n",
      "iter 106 ---  Loss: 0.6929733856115491    Accuracy: 91.5625\n"
     ]
    }
   ],
   "source": [
    "# Let's do fine-tuning\n",
    "finetune_model(model=vgg16, data_loader=train_loader, num_epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.load_state_dict(torch.load(os.path.join(model_dict_path, \"vgg16_finetune_params.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would evaluate the model.\n",
    "\n",
    "Make sure it outputs all of the accuracies of all 20 conditions. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, data_loader, limit:int, device:str):\n",
    "    \"\"\"\n",
    "    Instance method that would evaluate with a given\n",
    "    data loader, the accuracies obtained by the VGGNET16\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    #Use no grad to not perform backpropagation for inference time\n",
    "    with torch.no_grad():\n",
    "        #Iterate through each of the images and labels\n",
    "        \n",
    "        # Calculate the total numbers for metrics\n",
    "        TP, FP, TN, FN = 0.0, 0.0, 0.0, 0.0\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "    \n",
    "            #See if it works\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            #Print the shape of each one of them\n",
    "            print(f\"Inputs shape: {images_inputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "            #Send the outputs to model in device\n",
    "            outputs = model(images_inputs)\n",
    "\n",
    "            #Binarize the output with threshold\n",
    "            pred_labels = (outputs > threshold).float()\n",
    "\n",
    "            # Calculate batch-wise TP, FP, TN, FN\n",
    "            b_TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            b_FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            b_TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            b_FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            TP += b_TP\n",
    "            FP += b_FP\n",
    "            TN += b_TN\n",
    "            FN += b_FN\n",
    "\n",
    "        #_, predicted = torch.max(outputs, 1)  # Get the index of the maximum log-probability\n",
    "        accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0\n",
    "        precision = (TP / (TP + FP)) * 100.0 if (TP + FP) > 0 else 0.0\n",
    "        recall = (TP / (TP + FN)) * 100.0 if (TP + FN) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "        print(\"Precision: {:.2f}%\".format(precision))\n",
    "        print(\"Recall: {:.2f}%\".format(recall))\n",
    "        print(\"F1 Score: {:.2f}%\".format(f1_score))\n",
    "\n",
    "            # accuracies.append(accuracy)\n",
    "            # precisions.append(precision)\n",
    "            # recalls.append(recall)\n",
    "            # f1_scores.append(f1_score)\n",
    "\n",
    "            # if idx == limit:\n",
    "            #     print(\"Limit reached\")\n",
    "            #     break\n",
    "    return accuracies, precisions, recalls, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the eval set\n",
    "accuracies, precisions, recalls, f1_scores = evaluate_model(vgg16, test_loader, 5, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
