{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Pre trained to test\n",
    "\n",
    "This jupyter notebook has the objective to, not only retrieve the accuracies of the VGGnet16 pretrained, but to obtain also <br>\n",
    "the layer features before the last classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/cs231/CS231N-Final-Proj/notebooks\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "notebooks_path = os.getcwd() # OR MAYBE has to be set manually depending your computer\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(notebooks_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "df_path = os.path.join(notebooks_path, os.pardir, \"data\")\n",
    "data_path = os.path.join(df_path, \"images\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 22.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(df_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the images and labels\n",
    "\n",
    "Also, obtain DataLoaders for test, train, and validation datasets using <br>\n",
    "the Dataloader class from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.CenterCrop((224, 224)),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 32, and shuffle true, and num workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vgg16 pretrained model\n",
    "\n",
    "Check if you have GPU Envidia! Else, use the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Load the pretrained model\n",
    "vgg16 = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the classifier layers\n",
    "We know that VGGnet16 has a last linear layer with 1000 output units...<br>\n",
    "However, this doesnt really resemble our problem per se...<br><br>\n",
    "Lets do this! Lets replace the last layer with a linear layer that has the same <br> number of classes as our data!. (In our case, is 20).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.features architecture and get the parameter shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64]), torch.Size([64, 64, 3, 3]), torch.Size([64]), torch.Size([128, 64, 3, 3]), torch.Size([128]), torch.Size([128, 128, 3, 3]), torch.Size([128]), torch.Size([256, 128, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([512, 256, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.features)\n",
    "print([x.shape for x in vgg16.features.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.avgpool parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.avgpool)\n",
    "print([x.shape for x in vgg16.avgpool.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16 classifier parameters and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "[torch.Size([4096, 25088]), torch.Size([4096]), torch.Size([4096, 4096]), torch.Size([4096]), torch.Size([1000, 4096]), torch.Size([1000])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.classifier)\n",
    "print([x.shape for x in vgg16.classifier.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL to conduct fine-tuning on Vggnet16 only on the last (Linear) layer\n",
    "\n",
    "# First, freeze all the parameters\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.1, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.1, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=20, bias=True)\n",
      "  (7): ELU(alpha=1.0, inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the last layer for the last 20 classes\n",
    "num_classes = 20  # Number of classes for your specific task\n",
    "num_features = vgg16.classifier[0].in_features #Get all of the features after convolutional layers\n",
    "print(num_features)\n",
    "\n",
    "#Obtain the same classifier you got befor with lower number of classes, so we can pretrain it\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 4096, bias=True),\n",
    "    nn.ReLU(inplace= True),\n",
    "    nn.Dropout(0.1, inplace=False),\n",
    "    nn.Linear(4096, 4096, bias=True),\n",
    "    nn.ReLU(inplace= True),\n",
    "    nn.Dropout(0.1, inplace=False),\n",
    "    nn.Linear(4096, num_classes, bias=True),\n",
    "    nn.ELU(inplace=True)\n",
    ")\n",
    "\n",
    "print(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "# Create state_dict path\n",
    "model_dict_path = os.path.join(notebooks_path, os.pardir, \"models\")\n",
    "\n",
    "if os.path.exists(model_dict_path) == False:\n",
    "    os.mkdir(model_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object Module.parameters at 0x77a1c9ca8ac0>]\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL to perform fine-tuning\n",
    "#print([x.shape for x in vgg16.classifier[-6].parameters()])\n",
    "\n",
    "import torch.optim as optim\n",
    "vgg16 = vgg16.to(device)\n",
    "vgg16.train()\n",
    "params_to_update = [vgg16.classifier.parameters()]\n",
    "print(params_to_update)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(*params_to_update, lr=0.01)\n",
    "\n",
    "def finetune_model(model, data_loader, num_epochs, device:str):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-------------')\n",
    "            \n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            # Convert labels to float type (also need to move to CUDA again!)\n",
    "            images_labels = images_labels.to(torch.float64)\n",
    "\n",
    "            # initialize optimizer\n",
    "            optimizer.zero_grad()            \n",
    "            outputs = model(images_inputs)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(outputs, images_labels)\n",
    "            \n",
    "            # predict labels\n",
    "            pred_labels = (outputs > 0.5).float()\n",
    "\n",
    "            # Calculate TP, FP, TN, FN and accuracy\n",
    "            TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0                \n",
    "\n",
    "            loss.backward()\n",
    "            print(f\"iter {idx} ---  Loss: {loss}    Accuracy: {accuracy}\")\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save parameters for each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_dict_path, \"vgg16_finetune_params.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------\n",
      "iter 0 ---  Loss: 4.602937683463097    Accuracy: 92.34375\n",
      "iter 1 ---  Loss: 4.0355129688978195    Accuracy: 93.125\n",
      "iter 2 ---  Loss: 5.280447103083134    Accuracy: 90.9375\n",
      "iter 3 ---  Loss: 4.325652673840523    Accuracy: 92.5\n",
      "iter 4 ---  Loss: 3.2509279623627663    Accuracy: 93.125\n",
      "iter 5 ---  Loss: 3.7702549137175083    Accuracy: 90.3125\n",
      "iter 6 ---  Loss: 3.9184491150081158    Accuracy: 86.875\n",
      "iter 7 ---  Loss: 3.3907826878130436    Accuracy: 86.25\n",
      "iter 8 ---  Loss: 3.558126490563154    Accuracy: 85.9375\n",
      "iter 9 ---  Loss: 3.810967568308115    Accuracy: 86.25\n",
      "iter 10 ---  Loss: 3.9466537050902843    Accuracy: 84.375\n",
      "iter 11 ---  Loss: 2.882425855845213    Accuracy: 84.84375\n",
      "iter 12 ---  Loss: 3.843246625736356    Accuracy: 82.34375\n",
      "iter 13 ---  Loss: 3.1304370388388634    Accuracy: 80.15625\n",
      "iter 14 ---  Loss: 3.4035610891878605    Accuracy: 78.90625\n",
      "iter 15 ---  Loss: 4.175964046269655    Accuracy: 80.0\n",
      "iter 16 ---  Loss: 4.340290382504463    Accuracy: 76.40625\n",
      "iter 17 ---  Loss: 3.9153260588645935    Accuracy: 75.9375\n",
      "iter 18 ---  Loss: 2.9138485975563526    Accuracy: 77.1875\n",
      "iter 19 ---  Loss: 4.91011680662632    Accuracy: 76.25\n",
      "iter 20 ---  Loss: 4.08153748139739    Accuracy: 75.625\n",
      "iter 21 ---  Loss: 3.525646772235632    Accuracy: 76.25\n",
      "iter 22 ---  Loss: 2.9714224711060524    Accuracy: 78.125\n",
      "iter 23 ---  Loss: 4.131732705980539    Accuracy: 75.15625\n",
      "iter 24 ---  Loss: 4.383557680994272    Accuracy: 75.625\n",
      "iter 25 ---  Loss: 3.479421067982912    Accuracy: 78.90625\n",
      "iter 26 ---  Loss: 3.832069180905819    Accuracy: 78.90625\n",
      "iter 27 ---  Loss: 4.218684423714876    Accuracy: 77.65625\n",
      "iter 28 ---  Loss: 2.6378659829497337    Accuracy: 78.59375\n",
      "iter 29 ---  Loss: 4.496872883290052    Accuracy: 76.71875\n",
      "iter 30 ---  Loss: 3.5206950940191746    Accuracy: 71.5625\n",
      "iter 31 ---  Loss: 4.021013891324401    Accuracy: 72.1875\n",
      "iter 32 ---  Loss: 2.526434399187565    Accuracy: 71.5625\n",
      "iter 33 ---  Loss: 4.204467341303825    Accuracy: 69.0625\n",
      "iter 34 ---  Loss: 3.610989283770323    Accuracy: 68.59375\n",
      "iter 35 ---  Loss: 4.293649356812239    Accuracy: 70.0\n",
      "iter 36 ---  Loss: 4.12847838178277    Accuracy: 68.75\n",
      "iter 37 ---  Loss: 3.527122378349304    Accuracy: 73.4375\n",
      "iter 38 ---  Loss: 3.03954765945673    Accuracy: 69.53125\n",
      "iter 39 ---  Loss: 3.367300346493721    Accuracy: 65.78125\n",
      "iter 40 ---  Loss: 3.211511593312025    Accuracy: 71.40625\n",
      "iter 41 ---  Loss: 3.8810756243765354    Accuracy: 70.0\n",
      "iter 42 ---  Loss: 3.4846932888031006    Accuracy: 67.8125\n",
      "iter 43 ---  Loss: 2.193112801760435    Accuracy: 66.875\n",
      "iter 44 ---  Loss: 4.680071264505386    Accuracy: 63.59375000000001\n",
      "iter 45 ---  Loss: 3.241377767175436    Accuracy: 70.46875\n",
      "iter 46 ---  Loss: 2.793841013684869    Accuracy: 68.59375\n",
      "iter 47 ---  Loss: 3.44224257953465    Accuracy: 65.46875\n",
      "iter 48 ---  Loss: 3.7710101883858442    Accuracy: 66.40625\n",
      "iter 49 ---  Loss: 3.760899033397436    Accuracy: 61.5625\n",
      "iter 50 ---  Loss: 3.632475696504116    Accuracy: 62.65625\n",
      "iter 51 ---  Loss: 3.501073144376278    Accuracy: 61.875\n",
      "iter 52 ---  Loss: 4.118312582373619    Accuracy: 62.18749999999999\n",
      "iter 53 ---  Loss: 3.1705547384917736    Accuracy: 60.0\n"
     ]
    }
   ],
   "source": [
    "# Let's do fine-tuning\n",
    "finetune_model(model=vgg16, data_loader=train_loader, num_epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.load_state_dict(torch.load(os.path.join(model_dict_path, \"vgg16_finetune_params.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would evaluate the model.\n",
    "\n",
    "Make sure it outputs all of the accuracies of all 20 conditions. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, data_loader, limit:int, device:str):\n",
    "    \"\"\"\n",
    "    Instance method that would evaluate with a given\n",
    "    data loader, the accuracies obtained by the VGGNET16\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    #Use no grad to not perform backpropagation for inference time\n",
    "    with torch.no_grad():\n",
    "        #Iterate through each of the images and labels\n",
    "        \n",
    "        # Calculate the total numbers for metrics\n",
    "        TP, FP, TN, FN = 0.0, 0.0, 0.0, 0.0\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "    \n",
    "            #See if it works\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            #Print the shape of each one of them\n",
    "            print(f\"Inputs shape: {images_inputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "            #Send the outputs to model in device\n",
    "            outputs = model(images_inputs)\n",
    "\n",
    "            #Binarize the output with threshold\n",
    "            pred_labels = (outputs > threshold).float()\n",
    "\n",
    "            # Calculate batch-wise TP, FP, TN, FN\n",
    "            b_TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            b_FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            b_TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            b_FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            TP += b_TP\n",
    "            FP += b_FP\n",
    "            TN += b_TN\n",
    "            FN += b_FN\n",
    "\n",
    "        #_, predicted = torch.max(outputs, 1)  # Get the index of the maximum log-probability\n",
    "        accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0\n",
    "        precision = (TP / (TP + FP)) * 100.0 if (TP + FP) > 0 else 0.0\n",
    "        recall = (TP / (TP + FN)) * 100.0 if (TP + FN) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "        print(\"Precision: {:.2f}%\".format(precision))\n",
    "        print(\"Recall: {:.2f}%\".format(recall))\n",
    "        print(\"F1 Score: {:.2f}%\".format(f1_score))\n",
    "\n",
    "            # accuracies.append(accuracy)\n",
    "            # precisions.append(precision)\n",
    "            # recalls.append(recall)\n",
    "            # f1_scores.append(f1_score)\n",
    "\n",
    "            # if idx == limit:\n",
    "            #     print(\"Limit reached\")\n",
    "            #     break\n",
    "    return accuracies, precisions, recalls, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the eval set\n",
    "accuracies, precisions, recalls, f1_scores = evaluate_model(vgg16, test_loader, 5, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
