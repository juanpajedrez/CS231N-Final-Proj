{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Pre trained to test\n",
    "\n",
    "This jupyter notebook has the objective to, not only retrieve the accuracies of the VGGnet16 pretrained, but to obtain also <br>\n",
    "the layer features before the last classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/cs231/CS231N-Final-Proj/notebooks\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "notebooks_path = os.getcwd() # OR MAYBE has to be set manually depending your computer\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(notebooks_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "df_path = os.path.join(notebooks_path, os.pardir, \"data\")\n",
    "data_path = os.path.join(df_path, \"images\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 24.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(df_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the images and labels\n",
    "\n",
    "Also, obtain DataLoaders for test, train, and validation datasets using <br>\n",
    "the Dataloader class from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([32, 3, 224, 224]), labels: torch.Size([32, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.CenterCrop((224, 224)),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 32, and shuffle true, and num workers = 4\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vgg16 pretrained model\n",
    "\n",
    "Check if you have GPU Envidia! Else, use the cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/juan/.conda/envs/cs231n/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/juan/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:02<00:00, 190MB/s]  \n"
     ]
    }
   ],
   "source": [
    "#Load the pretrained model\n",
    "vgg16 = models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the classifier layers\n",
    "We know that VGGnet16 has a last linear layer with 1000 output units...<br>\n",
    "However, this doesnt really resemble our problem per se...<br><br>\n",
    "Lets do this! Lets replace the last layer with a linear layer that has the same <br> number of classes as our data!. (In our case, is 20).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.features architecture and get the parameter shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "[torch.Size([64, 3, 3, 3]), torch.Size([64]), torch.Size([64, 64, 3, 3]), torch.Size([64]), torch.Size([128, 64, 3, 3]), torch.Size([128]), torch.Size([128, 128, 3, 3]), torch.Size([128]), torch.Size([256, 128, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([256, 256, 3, 3]), torch.Size([256]), torch.Size([512, 256, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512]), torch.Size([512, 512, 3, 3]), torch.Size([512])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.features)\n",
    "print([x.shape for x in vgg16.features.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16.avgpool parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.avgpool)\n",
    "print([x.shape for x in vgg16.avgpool.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the vgg16 classifier parameters and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "[torch.Size([4096, 25088]), torch.Size([4096]), torch.Size([4096, 4096]), torch.Size([4096]), torch.Size([1000, 4096]), torch.Size([1000])]\n"
     ]
    }
   ],
   "source": [
    "print(vgg16.classifier)\n",
    "print([x.shape for x in vgg16.classifier.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL to conduct fine-tuning on Vggnet16 only on the last (Linear) layer\n",
    "\n",
    "# First, freeze all the parameters\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25088\n",
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=20, bias=True)\n",
      "  (7): ELU(alpha=1.0, inplace=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modify the last layer for the last 20 classes\n",
    "num_classes = 20  # Number of classes for your specific task\n",
    "num_features = vgg16.classifier[0].in_features #Get all of the features after convolutional layers\n",
    "print(num_features)\n",
    "\n",
    "#Obtain the same classifier you got befor with lower number of classes, so we can pretrain it\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(num_features, 4096, bias=True),\n",
    "    nn.ReLU(inplace= True),\n",
    "    nn.Dropout(0.1, inplace=False),\n",
    "    nn.Linear(4096, 4096, bias=True),\n",
    "    nn.ReLU(inplace= True),\n",
    "    nn.Dropout(0.1, inplace=False),\n",
    "    nn.Linear(4096, num_classes, bias=True),\n",
    "    nn.ELU(inplace=True)\n",
    ")\n",
    "\n",
    "print(vgg16.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW CODE CELL\n",
    "\n",
    "# Create state_dict path\n",
    "model_dict_path = os.path.join(notebooks_path, os.pardir, \"models\")\n",
    "\n",
    "if os.path.exists(model_dict_path) == False:\n",
    "    os.mkdir(model_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object Module.parameters at 0x77a1c9deb300>]\n"
     ]
    }
   ],
   "source": [
    "# NEW CODE CELL to perform fine-tuning\n",
    "#print([x.shape for x in vgg16.classifier[-6].parameters()])\n",
    "\n",
    "import torch.optim as optim\n",
    "vgg16 = vgg16.to(device)\n",
    "vgg16.train()\n",
    "params_to_update = [vgg16.classifier.parameters()]\n",
    "print(params_to_update)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(*params_to_update, lr=0.01)\n",
    "\n",
    "def finetune_model(model, data_loader, num_epochs, device:str):\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-------------')\n",
    "            \n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            # Convert labels to float type (also need to move to CUDA again!)\n",
    "            images_labels = images_labels.to(torch.float64)\n",
    "\n",
    "            # initialize optimizer\n",
    "            optimizer.zero_grad()            \n",
    "            outputs = model(images_inputs)\n",
    "            \n",
    "            # compute loss\n",
    "            loss = criterion(outputs, images_labels)\n",
    "            \n",
    "            # predict labels\n",
    "            pred_labels = (outputs > 0.5).float()\n",
    "\n",
    "            # Calculate TP, FP, TN, FN and accuracy\n",
    "            TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0                \n",
    "\n",
    "            loss.backward()\n",
    "            print(f\"iter {idx} ---  Loss: {loss}    Accuracy: {accuracy}\")\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save parameters for each epoch\n",
    "        torch.save(model.state_dict(), os.path.join(model_dict_path, \"vgg16_finetune_params.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "-------------\n",
      "iter 0 ---  Loss: 4.827276900410652    Accuracy: 91.875\n",
      "iter 1 ---  Loss: 4.120321191847324    Accuracy: 92.8125\n",
      "iter 2 ---  Loss: 3.631840765476227    Accuracy: 93.28125\n",
      "iter 3 ---  Loss: 3.9102262631058693    Accuracy: 91.71875\n",
      "iter 4 ---  Loss: 4.765174761414528    Accuracy: 88.4375\n",
      "iter 5 ---  Loss: 4.188632417470217    Accuracy: 88.59375\n",
      "iter 6 ---  Loss: 3.723295398056507    Accuracy: 89.21875\n",
      "iter 7 ---  Loss: 3.839276373386383    Accuracy: 88.59375\n",
      "iter 8 ---  Loss: 3.9545043259859085    Accuracy: 86.25\n",
      "iter 9 ---  Loss: 3.7797516509890556    Accuracy: 85.9375\n",
      "iter 10 ---  Loss: 3.344309937208891    Accuracy: 85.46875\n",
      "iter 11 ---  Loss: 3.6283507645130157    Accuracy: 85.625\n",
      "iter 12 ---  Loss: 4.6351980548352    Accuracy: 85.625\n",
      "iter 13 ---  Loss: 3.3206539675593376    Accuracy: 82.65625\n",
      "iter 14 ---  Loss: 4.11974523589015    Accuracy: 80.625\n",
      "iter 15 ---  Loss: 3.8524534702301025    Accuracy: 80.78125\n",
      "iter 16 ---  Loss: 3.7188658379018307    Accuracy: 79.375\n",
      "iter 17 ---  Loss: 3.6082465015351772    Accuracy: 81.09375\n",
      "iter 18 ---  Loss: 3.6135971769690514    Accuracy: 79.6875\n",
      "iter 19 ---  Loss: 3.9419322311878204    Accuracy: 77.8125\n",
      "iter 20 ---  Loss: 3.7034739926457405    Accuracy: 76.25\n",
      "iter 21 ---  Loss: 3.016524977982044    Accuracy: 78.90625\n",
      "iter 22 ---  Loss: 3.141789088025689    Accuracy: 77.96875\n",
      "iter 23 ---  Loss: 3.080797929316759    Accuracy: 78.28125\n",
      "iter 24 ---  Loss: 4.118655188009143    Accuracy: 78.90625\n",
      "iter 25 ---  Loss: 2.764735508710146    Accuracy: 76.71875\n",
      "iter 26 ---  Loss: 3.640829173848033    Accuracy: 76.09375\n",
      "iter 27 ---  Loss: 3.5013767536729574    Accuracy: 77.65625\n",
      "iter 28 ---  Loss: 4.6819965448230505    Accuracy: 77.96875\n",
      "iter 29 ---  Loss: 4.436807256191969    Accuracy: 75.9375\n",
      "iter 30 ---  Loss: 4.774013604968786    Accuracy: 74.6875\n",
      "iter 31 ---  Loss: 4.280974727123976    Accuracy: 75.46875\n",
      "iter 32 ---  Loss: 3.1434852220118046    Accuracy: 77.34375\n",
      "iter 33 ---  Loss: 4.404196850955486    Accuracy: 74.6875\n",
      "iter 34 ---  Loss: 3.456439759582281    Accuracy: 78.125\n",
      "iter 35 ---  Loss: 3.6643856838345528    Accuracy: 77.5\n",
      "iter 36 ---  Loss: 3.2898314967751503    Accuracy: 75.78125\n",
      "iter 37 ---  Loss: 2.967075714841485    Accuracy: 74.53125\n",
      "iter 38 ---  Loss: 3.7857145853340626    Accuracy: 73.59375\n",
      "iter 39 ---  Loss: 3.877197578549385    Accuracy: 70.15625\n",
      "iter 40 ---  Loss: 3.707360878586769    Accuracy: 69.0625\n",
      "iter 41 ---  Loss: 3.2074261754751205    Accuracy: 68.75\n",
      "iter 42 ---  Loss: 2.96956817060709    Accuracy: 70.15625\n",
      "iter 43 ---  Loss: 3.819629780948162    Accuracy: 68.75\n",
      "iter 44 ---  Loss: 3.7395562827587128    Accuracy: 65.78125\n",
      "iter 45 ---  Loss: 3.6710685826838017    Accuracy: 67.8125\n",
      "iter 46 ---  Loss: 3.3652738854289055    Accuracy: 70.625\n",
      "iter 47 ---  Loss: 3.067745730280876    Accuracy: 73.4375\n",
      "iter 48 ---  Loss: 4.278639271855354    Accuracy: 68.75\n",
      "iter 49 ---  Loss: 4.042565256357193    Accuracy: 65.78125\n",
      "iter 50 ---  Loss: 3.421560686081648    Accuracy: 65.0\n",
      "iter 51 ---  Loss: 3.0203063301742077    Accuracy: 67.03125\n",
      "iter 52 ---  Loss: 3.2248023245483637    Accuracy: 68.28125\n",
      "iter 53 ---  Loss: 3.2979225888848305    Accuracy: 68.90625\n",
      "iter 54 ---  Loss: 3.4154911935329437    Accuracy: 66.09375\n",
      "iter 55 ---  Loss: 2.876629287377    Accuracy: 68.125\n",
      "iter 56 ---  Loss: 3.519939938560128    Accuracy: 67.8125\n",
      "iter 57 ---  Loss: 3.624436007812619    Accuracy: 70.625\n",
      "iter 58 ---  Loss: 3.6952834762632847    Accuracy: 65.625\n",
      "iter 59 ---  Loss: 3.9297516979277134    Accuracy: 66.5625\n",
      "iter 60 ---  Loss: 4.110872384160757    Accuracy: 63.74999999999999\n",
      "iter 61 ---  Loss: 3.762720998376608    Accuracy: 64.6875\n",
      "iter 62 ---  Loss: 3.829918272793293    Accuracy: 65.0\n",
      "iter 63 ---  Loss: 4.505214914679527    Accuracy: 66.5625\n",
      "iter 64 ---  Loss: 3.400542564690113    Accuracy: 62.5\n",
      "iter 65 ---  Loss: 4.247326049953699    Accuracy: 66.40625\n",
      "iter 66 ---  Loss: 3.968030657619238    Accuracy: 65.46875\n",
      "iter 67 ---  Loss: 2.8984583504498005    Accuracy: 61.875\n",
      "iter 68 ---  Loss: 2.5740817822515965    Accuracy: 65.625\n",
      "iter 69 ---  Loss: 4.6191638465970755    Accuracy: 64.6875\n",
      "iter 70 ---  Loss: 3.3712800666689873    Accuracy: 64.84375\n",
      "iter 71 ---  Loss: 2.8829939123243093    Accuracy: 65.46875\n",
      "iter 72 ---  Loss: 4.298568308353424    Accuracy: 64.84375\n",
      "iter 73 ---  Loss: 2.8950488306581974    Accuracy: 62.65625\n",
      "iter 74 ---  Loss: 3.7618192974478006    Accuracy: 64.53125\n",
      "iter 75 ---  Loss: 3.9450993798673153    Accuracy: 60.9375\n",
      "iter 76 ---  Loss: 3.8421495612710714    Accuracy: 61.875\n",
      "iter 77 ---  Loss: 3.5915307514369488    Accuracy: 61.40624999999999\n",
      "iter 78 ---  Loss: 3.3590817973017693    Accuracy: 62.65625\n",
      "iter 79 ---  Loss: 3.183570012450218    Accuracy: 66.25\n",
      "iter 80 ---  Loss: 4.805235156789422    Accuracy: 67.5\n",
      "iter 81 ---  Loss: 2.4355505369603634    Accuracy: 60.15625\n",
      "iter 82 ---  Loss: 3.6481271497905254    Accuracy: 64.6875\n",
      "iter 83 ---  Loss: 2.9525969438254833    Accuracy: 61.71875\n",
      "iter 84 ---  Loss: 3.76738141477108    Accuracy: 61.875\n",
      "iter 85 ---  Loss: 2.408724645152688    Accuracy: 59.68750000000001\n",
      "iter 86 ---  Loss: 3.2327346242964268    Accuracy: 62.5\n",
      "iter 87 ---  Loss: 3.9147640354931355    Accuracy: 61.71875\n",
      "iter 88 ---  Loss: 4.223318640142679    Accuracy: 60.62499999999999\n",
      "iter 89 ---  Loss: 3.4761102590709925    Accuracy: 63.90625\n",
      "iter 90 ---  Loss: 3.4134394209831953    Accuracy: 62.34375\n",
      "iter 91 ---  Loss: 3.877333842217922    Accuracy: 62.5\n",
      "iter 92 ---  Loss: 3.8408929631114006    Accuracy: 60.46875000000001\n",
      "iter 93 ---  Loss: 3.3338321335613728    Accuracy: 60.3125\n",
      "iter 94 ---  Loss: 2.847843337804079    Accuracy: 62.96874999999999\n",
      "iter 95 ---  Loss: 2.6026012804359198    Accuracy: 60.0\n",
      "iter 96 ---  Loss: 3.2665475141257048    Accuracy: 60.78125\n",
      "iter 97 ---  Loss: 2.8732575811445713    Accuracy: 61.875\n",
      "iter 98 ---  Loss: 3.379348259419203    Accuracy: 58.12500000000001\n",
      "iter 99 ---  Loss: 3.151189737021923    Accuracy: 59.68750000000001\n",
      "iter 100 ---  Loss: 4.0746433306485415    Accuracy: 62.34375\n",
      "iter 101 ---  Loss: 2.6375333443284035    Accuracy: 61.25000000000001\n",
      "iter 102 ---  Loss: 3.346911574713886    Accuracy: 61.25000000000001\n",
      "iter 103 ---  Loss: 3.8695765491575003    Accuracy: 63.125\n",
      "iter 104 ---  Loss: 3.547589749097824    Accuracy: 60.9375\n",
      "iter 105 ---  Loss: 3.71447566524148    Accuracy: 60.78125\n",
      "iter 106 ---  Loss: 4.060709312558174    Accuracy: 61.5625\n",
      "iter 107 ---  Loss: 3.3555865809321404    Accuracy: 62.34375\n",
      "iter 108 ---  Loss: 3.4731195718050003    Accuracy: 63.28125\n",
      "iter 109 ---  Loss: 3.8560703583061695    Accuracy: 61.40624999999999\n",
      "iter 110 ---  Loss: 3.2927802614867687    Accuracy: 59.375\n",
      "iter 111 ---  Loss: 3.3597420677542686    Accuracy: 60.3125\n",
      "iter 112 ---  Loss: 3.65337086096406    Accuracy: 60.62499999999999\n",
      "iter 113 ---  Loss: 3.821391724050045    Accuracy: 60.15625\n",
      "iter 114 ---  Loss: 3.5184745211154222    Accuracy: 61.25000000000001\n",
      "iter 115 ---  Loss: 3.2720463648438454    Accuracy: 62.96874999999999\n",
      "iter 116 ---  Loss: 2.8568993024528027    Accuracy: 60.15625\n",
      "iter 117 ---  Loss: 3.786155829206109    Accuracy: 61.71875\n",
      "iter 118 ---  Loss: 3.4434034284204245    Accuracy: 60.78125\n",
      "iter 119 ---  Loss: 3.9812548123300076    Accuracy: 60.78125\n",
      "iter 120 ---  Loss: 3.4304361399263144    Accuracy: 57.1875\n",
      "iter 121 ---  Loss: 3.077018368989229    Accuracy: 58.75\n",
      "iter 122 ---  Loss: 3.232302650809288    Accuracy: 61.40624999999999\n",
      "iter 123 ---  Loss: 3.217694219201803    Accuracy: 61.40624999999999\n",
      "iter 124 ---  Loss: 3.670368619263172    Accuracy: 59.21875\n",
      "iter 125 ---  Loss: 3.324285477399826    Accuracy: 58.90625000000001\n",
      "iter 126 ---  Loss: 4.542190607637167    Accuracy: 62.03125000000001\n",
      "iter 127 ---  Loss: 3.6130841970443726    Accuracy: 62.03125000000001\n",
      "iter 128 ---  Loss: 3.6059873830527067    Accuracy: 60.62499999999999\n",
      "iter 129 ---  Loss: 3.4310169722884893    Accuracy: 59.84374999999999\n",
      "iter 130 ---  Loss: 4.785031363368034    Accuracy: 61.25000000000001\n",
      "iter 131 ---  Loss: 3.4564469158649445    Accuracy: 60.0\n",
      "iter 132 ---  Loss: 3.429329624399543    Accuracy: 60.9375\n",
      "iter 133 ---  Loss: 3.179880252107978    Accuracy: 61.5625\n",
      "iter 134 ---  Loss: 4.660934653133154    Accuracy: 60.15625\n",
      "iter 135 ---  Loss: 3.138908378779888    Accuracy: 60.46875000000001\n",
      "iter 136 ---  Loss: 3.0164293814450502    Accuracy: 62.81250000000001\n",
      "iter 137 ---  Loss: 3.144772717729211    Accuracy: 63.125\n",
      "iter 138 ---  Loss: 3.6870490685105324    Accuracy: 62.65625\n",
      "iter 139 ---  Loss: 4.638909727334976    Accuracy: 63.4375\n",
      "iter 140 ---  Loss: 3.10539560765028    Accuracy: 59.21875\n",
      "iter 141 ---  Loss: 3.094055036082864    Accuracy: 59.53125\n",
      "iter 142 ---  Loss: 2.81614656932652    Accuracy: 58.12500000000001\n",
      "iter 143 ---  Loss: 3.833882031030953    Accuracy: 59.375\n",
      "iter 144 ---  Loss: 3.3353880662471056    Accuracy: 59.68750000000001\n",
      "iter 145 ---  Loss: 3.5451661478728056    Accuracy: 57.03125\n",
      "iter 146 ---  Loss: 3.869952967390418    Accuracy: 57.03125\n",
      "iter 147 ---  Loss: 3.9470910392701626    Accuracy: 58.90625000000001\n",
      "iter 148 ---  Loss: 3.4514897167682648    Accuracy: 53.90625\n",
      "iter 149 ---  Loss: 3.21483950689435    Accuracy: 53.90625\n",
      "iter 150 ---  Loss: 4.048076905310154    Accuracy: 51.5625\n",
      "iter 151 ---  Loss: 3.707805372774601    Accuracy: 51.40625\n",
      "iter 152 ---  Loss: 4.119705758988857    Accuracy: 53.28125\n",
      "iter 153 ---  Loss: 3.1501621063798666    Accuracy: 50.46874999999999\n",
      "iter 154 ---  Loss: 3.841171571984887    Accuracy: 49.0625\n",
      "iter 155 ---  Loss: 3.5010864716023207    Accuracy: 48.28125\n",
      "iter 156 ---  Loss: 4.5335732735693455    Accuracy: 51.09375000000001\n",
      "iter 157 ---  Loss: 3.690820910036564    Accuracy: 48.125\n",
      "iter 158 ---  Loss: 3.3049767035990953    Accuracy: 50.9375\n",
      "iter 159 ---  Loss: 4.132610103115439    Accuracy: 51.5625\n",
      "iter 160 ---  Loss: 3.283856637775898    Accuracy: 51.71875\n",
      "iter 161 ---  Loss: 3.449455562978983    Accuracy: 51.24999999999999\n",
      "iter 162 ---  Loss: 2.9945373330265284    Accuracy: 52.81249999999999\n",
      "iter 163 ---  Loss: 3.9094369541853666    Accuracy: 51.87500000000001\n",
      "iter 164 ---  Loss: 2.9544739332050085    Accuracy: 51.24999999999999\n",
      "iter 165 ---  Loss: 3.158687023445964    Accuracy: 53.43750000000001\n",
      "iter 166 ---  Loss: 3.8596126697957516    Accuracy: 52.34375\n",
      "iter 167 ---  Loss: 3.159378230571747    Accuracy: 52.5\n",
      "iter 168 ---  Loss: 3.1083110198378563    Accuracy: 52.5\n",
      "iter 169 ---  Loss: 3.6008298248052597    Accuracy: 53.75\n",
      "iter 170 ---  Loss: 3.3382732551544905    Accuracy: 52.5\n",
      "iter 171 ---  Loss: 3.5700729992240667    Accuracy: 53.28125\n",
      "iter 172 ---  Loss: 3.956576209515333    Accuracy: 53.59374999999999\n",
      "iter 173 ---  Loss: 2.632876142859459    Accuracy: 51.5625\n",
      "iter 174 ---  Loss: 3.795093707740307    Accuracy: 50.15625\n",
      "iter 175 ---  Loss: 3.1479348614811897    Accuracy: 50.15625\n",
      "iter 176 ---  Loss: 3.9545989222824574    Accuracy: 50.625\n",
      "iter 177 ---  Loss: 3.4011852853000164    Accuracy: 46.875\n",
      "iter 178 ---  Loss: 3.0039053186774254    Accuracy: 48.75\n",
      "iter 179 ---  Loss: 3.931007992476225    Accuracy: 51.5625\n",
      "iter 180 ---  Loss: 3.330286642536521    Accuracy: 48.125\n",
      "iter 181 ---  Loss: 2.7689961344003677    Accuracy: 50.15625\n",
      "iter 182 ---  Loss: 3.6788238398730755    Accuracy: 52.65625000000001\n",
      "iter 183 ---  Loss: 3.404306296259165    Accuracy: 49.6875\n",
      "iter 184 ---  Loss: 3.8947663381695747    Accuracy: 50.15625\n",
      "iter 185 ---  Loss: 3.410143829882145    Accuracy: 49.0625\n",
      "iter 186 ---  Loss: 2.9999267924576998    Accuracy: 44.6875\n",
      "iter 187 ---  Loss: 2.827170919626951    Accuracy: 46.40625\n",
      "iter 188 ---  Loss: 2.782032886520028    Accuracy: 45.46875\n",
      "iter 189 ---  Loss: 2.7634860686957836    Accuracy: 44.375\n",
      "iter 190 ---  Loss: 2.694134953431785    Accuracy: 46.5625\n",
      "iter 191 ---  Loss: 3.3083359603770077    Accuracy: 47.03125\n",
      "iter 192 ---  Loss: 3.5819702222943306    Accuracy: 45.9375\n",
      "iter 193 ---  Loss: 3.62687630020082    Accuracy: 47.96875\n",
      "iter 194 ---  Loss: 3.661098102107644    Accuracy: 46.71875\n",
      "iter 195 ---  Loss: 3.4178798217326403    Accuracy: 46.875\n",
      "iter 196 ---  Loss: 3.642571348696947    Accuracy: 47.65625\n",
      "iter 197 ---  Loss: 3.186394431628287    Accuracy: 47.1875\n",
      "iter 198 ---  Loss: 3.180925733409822    Accuracy: 45.9375\n",
      "iter 199 ---  Loss: 4.428300170227885    Accuracy: 46.5625\n",
      "iter 200 ---  Loss: 3.9465357176959515    Accuracy: 46.09375\n",
      "iter 201 ---  Loss: 2.901221614331007    Accuracy: 46.5625\n",
      "iter 202 ---  Loss: 3.8346977941691875    Accuracy: 47.65625\n",
      "iter 203 ---  Loss: 3.3797710966318846    Accuracy: 45.46875\n",
      "iter 204 ---  Loss: 3.955736370757222    Accuracy: 48.59375\n",
      "iter 205 ---  Loss: 3.414371222257614    Accuracy: 45.0\n",
      "iter 206 ---  Loss: 3.66471516340971    Accuracy: 42.65625\n",
      "iter 207 ---  Loss: 3.6439255364239216    Accuracy: 46.09375\n",
      "iter 208 ---  Loss: 3.290527893230319    Accuracy: 47.8125\n",
      "iter 209 ---  Loss: 3.0334875863045454    Accuracy: 47.96875\n",
      "iter 210 ---  Loss: 3.0073158401064575    Accuracy: 45.9375\n",
      "iter 211 ---  Loss: 3.3869455279782414    Accuracy: 49.6875\n",
      "iter 212 ---  Loss: 3.347394924610853    Accuracy: 46.71875\n",
      "iter 213 ---  Loss: 3.1910445988178253    Accuracy: 47.96875\n",
      "iter 214 ---  Loss: 2.8985183434560895    Accuracy: 48.75\n",
      "iter 215 ---  Loss: 3.2114324420690536    Accuracy: 48.90625\n",
      "iter 216 ---  Loss: 3.233087971806526    Accuracy: 50.15625\n",
      "iter 217 ---  Loss: 3.80361389555037    Accuracy: 51.09375000000001\n",
      "iter 218 ---  Loss: 3.5265173595398664    Accuracy: 50.31250000000001\n",
      "iter 219 ---  Loss: 3.2146933376789093    Accuracy: 51.09375000000001\n",
      "iter 220 ---  Loss: 4.176958739757538    Accuracy: 53.75\n",
      "iter 221 ---  Loss: 3.3581772334873676    Accuracy: 48.59375\n",
      "iter 222 ---  Loss: 4.324174489825964    Accuracy: 52.03124999999999\n",
      "iter 223 ---  Loss: 3.495573967695236    Accuracy: 48.75\n",
      "iter 224 ---  Loss: 3.780496255494654    Accuracy: 50.625\n",
      "iter 225 ---  Loss: 3.212008774280548    Accuracy: 50.9375\n",
      "iter 226 ---  Loss: 2.842859359458089    Accuracy: 50.0\n",
      "iter 227 ---  Loss: 3.524056618101895    Accuracy: 48.4375\n",
      "iter 228 ---  Loss: 3.152549546211958    Accuracy: 47.1875\n",
      "iter 229 ---  Loss: 2.7870651446282864    Accuracy: 45.9375\n",
      "iter 230 ---  Loss: 2.8262305604293942    Accuracy: 46.40625\n",
      "iter 231 ---  Loss: 2.839060854166746    Accuracy: 48.90625\n",
      "iter 232 ---  Loss: 3.177022489719093    Accuracy: 47.5\n",
      "iter 233 ---  Loss: 3.8272313000634313    Accuracy: 48.75\n",
      "iter 234 ---  Loss: 3.25681459531188    Accuracy: 47.34375\n",
      "iter 235 ---  Loss: 3.1622925754636526    Accuracy: 46.09375\n",
      "iter 236 ---  Loss: 3.7534214854240417    Accuracy: 46.40625\n",
      "iter 237 ---  Loss: 3.331088839098811    Accuracy: 47.5\n",
      "iter 238 ---  Loss: 3.265373792499304    Accuracy: 46.25\n",
      "iter 239 ---  Loss: 3.290649477392435    Accuracy: 47.5\n",
      "iter 240 ---  Loss: 2.876898266375065    Accuracy: 46.09375\n",
      "iter 241 ---  Loss: 3.6068360526114702    Accuracy: 46.40625\n",
      "iter 242 ---  Loss: 3.3106962330639362    Accuracy: 46.09375\n",
      "iter 243 ---  Loss: 3.1999458391219378    Accuracy: 47.65625\n",
      "iter 244 ---  Loss: 3.7046112790703773    Accuracy: 48.125\n",
      "iter 245 ---  Loss: 3.304398475214839    Accuracy: 48.125\n",
      "iter 246 ---  Loss: 4.514633206650615    Accuracy: 48.90625\n",
      "iter 247 ---  Loss: 3.6398608088493347    Accuracy: 45.78125\n",
      "iter 248 ---  Loss: 3.289365081116557    Accuracy: 45.9375\n",
      "iter 249 ---  Loss: 5.116439373232424    Accuracy: 50.0\n",
      "iter 250 ---  Loss: 3.0906758680939674    Accuracy: 47.03125\n",
      "iter 251 ---  Loss: 3.4826631601899862    Accuracy: 47.8125\n",
      "iter 252 ---  Loss: 2.9268475845456123    Accuracy: 49.21875\n",
      "iter 253 ---  Loss: 3.2564628440886736    Accuracy: 50.0\n",
      "iter 254 ---  Loss: 3.1454754639416933    Accuracy: 48.4375\n",
      "iter 255 ---  Loss: 4.060638453811407    Accuracy: 49.6875\n",
      "iter 256 ---  Loss: 2.885631412267685    Accuracy: 47.34375\n",
      "iter 257 ---  Loss: 2.094175547361374    Accuracy: 47.65625\n",
      "iter 258 ---  Loss: 3.599147972650826    Accuracy: 50.0\n",
      "iter 259 ---  Loss: 3.693840065971017    Accuracy: 49.375\n",
      "iter 260 ---  Loss: 2.7274406971409917    Accuracy: 50.0\n",
      "iter 261 ---  Loss: 3.0804786644876003    Accuracy: 52.1875\n",
      "iter 262 ---  Loss: 3.9383756332099438    Accuracy: 49.53125\n",
      "iter 263 ---  Loss: 3.21097394451499    Accuracy: 50.0\n",
      "iter 264 ---  Loss: 4.042193852365017    Accuracy: 50.625\n",
      "iter 265 ---  Loss: 3.395958472043276    Accuracy: 52.03124999999999\n",
      "iter 266 ---  Loss: 4.957760171033442    Accuracy: 55.15624999999999\n",
      "iter 267 ---  Loss: 2.935870984569192    Accuracy: 50.15625\n",
      "iter 268 ---  Loss: 3.5404498986899853    Accuracy: 54.6875\n",
      "iter 269 ---  Loss: 3.9591231625527143    Accuracy: 52.81249999999999\n",
      "iter 270 ---  Loss: 4.094828808680177    Accuracy: 50.15625\n",
      "iter 271 ---  Loss: 2.963732957839966    Accuracy: 51.09375000000001\n",
      "iter 272 ---  Loss: 3.102709957398474    Accuracy: 49.21875\n",
      "iter 273 ---  Loss: 3.460564326494932    Accuracy: 51.24999999999999\n",
      "iter 274 ---  Loss: 2.4471293967217207    Accuracy: 47.03125\n",
      "iter 275 ---  Loss: 3.447260459885001    Accuracy: 51.5625\n",
      "iter 276 ---  Loss: 4.518190495669842    Accuracy: 50.78125\n",
      "iter 277 ---  Loss: 2.5249914098531008    Accuracy: 49.53125\n",
      "iter 278 ---  Loss: 2.5536772082559764    Accuracy: 49.375\n",
      "iter 279 ---  Loss: 2.4971948196180165    Accuracy: 47.96875\n",
      "iter 280 ---  Loss: 3.610796400345862    Accuracy: 50.46874999999999\n",
      "iter 281 ---  Loss: 2.857962090522051    Accuracy: 46.71875\n",
      "iter 282 ---  Loss: 3.6802861923351884    Accuracy: 50.15625\n",
      "iter 283 ---  Loss: 3.074346696957946    Accuracy: 50.78125\n",
      "iter 284 ---  Loss: 3.704575337469578    Accuracy: 51.09375000000001\n",
      "iter 285 ---  Loss: 4.172525579109788    Accuracy: 50.15625\n",
      "iter 286 ---  Loss: 2.9577045142650604    Accuracy: 46.71875\n",
      "iter 287 ---  Loss: 3.193569593131542    Accuracy: 48.28125\n",
      "iter 288 ---  Loss: 3.356952991336584    Accuracy: 47.34375\n",
      "iter 289 ---  Loss: 3.5639373818412423    Accuracy: 49.0625\n",
      "iter 290 ---  Loss: 3.803709289059043    Accuracy: 50.625\n",
      "iter 291 ---  Loss: 3.9799853190779686    Accuracy: 49.84375\n",
      "iter 292 ---  Loss: 3.2097680140286684    Accuracy: 48.125\n",
      "iter 293 ---  Loss: 4.083917733281851    Accuracy: 49.21875\n",
      "iter 294 ---  Loss: 2.924307230859995    Accuracy: 46.40625\n",
      "iter 295 ---  Loss: 3.1828480809926987    Accuracy: 45.625\n",
      "iter 296 ---  Loss: 3.765221707522869    Accuracy: 46.5625\n",
      "iter 297 ---  Loss: 3.475448318757117    Accuracy: 47.1875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let's do fine-tuning\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfinetune_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvgg16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 35\u001b[0m, in \u001b[0;36mfinetune_model\u001b[0;34m(model, data_loader, num_epochs, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate TP, FP, TN, FN and accuracy\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m TP \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m FP \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((pred_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (images_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m TN \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((pred_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (images_labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's do fine-tuning\n",
    "finetune_model(model=vgg16, data_loader=train_loader, num_epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.load_state_dict(torch.load(os.path.join(model_dict_path, \"vgg16_finetune_params.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would evaluate the model.\n",
    "\n",
    "Make sure it outputs all of the accuracies of all 20 conditions. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, data_loader, limit:int, device:str):\n",
    "    \"\"\"\n",
    "    Instance method that would evaluate with a given\n",
    "    data loader, the accuracies obtained by the VGGNET16\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    #Use no grad to not perform backpropagation for inference time\n",
    "    with torch.no_grad():\n",
    "        #Iterate through each of the images and labels\n",
    "        \n",
    "        # Calculate the total numbers for metrics\n",
    "        TP, FP, TN, FN = 0.0, 0.0, 0.0, 0.0\n",
    "        for idx, batch in enumerate(data_loader):\n",
    "    \n",
    "            #See if it works\n",
    "            images_inputs, images_labels = batch\n",
    "            images_inputs, images_labels = images_inputs.to(device), images_labels.to(device)\n",
    "\n",
    "            #Print the shape of each one of them\n",
    "            print(f\"Inputs shape: {images_inputs.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "            #Send the outputs to model in device\n",
    "            outputs = model(images_inputs)\n",
    "\n",
    "            #Binarize the output with threshold\n",
    "            pred_labels = (outputs > threshold).float()\n",
    "\n",
    "            # Calculate batch-wise TP, FP, TN, FN\n",
    "            b_TP = torch.sum((pred_labels == 1) & (images_labels == 1)).item()\n",
    "            b_FP = torch.sum((pred_labels == 1) & (images_labels == 0)).item()\n",
    "            b_TN = torch.sum((pred_labels == 0) & (images_labels == 0)).item()\n",
    "            b_FN = torch.sum((pred_labels == 0) & (images_labels == 1)).item()\n",
    "            TP += b_TP\n",
    "            FP += b_FP\n",
    "            TN += b_TN\n",
    "            FN += b_FN\n",
    "\n",
    "        #_, predicted = torch.max(outputs, 1)  # Get the index of the maximum log-probability\n",
    "        accuracy = ((TP + TN) / (TP + FP + TN + FN)) * 100.0\n",
    "        precision = (TP / (TP + FP)) * 100.0 if (TP + FP) > 0 else 0.0\n",
    "        recall = (TP / (TP + FN)) * 100.0 if (TP + FN) > 0 else 0.0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "        print(\"Precision: {:.2f}%\".format(precision))\n",
    "        print(\"Recall: {:.2f}%\".format(recall))\n",
    "        print(\"F1 Score: {:.2f}%\".format(f1_score))\n",
    "\n",
    "            # accuracies.append(accuracy)\n",
    "            # precisions.append(precision)\n",
    "            # recalls.append(recall)\n",
    "            # f1_scores.append(f1_score)\n",
    "\n",
    "            # if idx == limit:\n",
    "            #     print(\"Limit reached\")\n",
    "            #     break\n",
    "    return accuracies, precisions, recalls, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the eval set\n",
    "accuracies, precisions, recalls, f1_scores = evaluate_model(vgg16, test_loader, 5, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
