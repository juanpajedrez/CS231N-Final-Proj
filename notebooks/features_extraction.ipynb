{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet16 features extraction.\n",
    "\n",
    "One of the key components to be able to create a GMVAE of the CXR14 dataset, is to be able to retrieve <br>\n",
    "the VGGne16 features, just as how the paper. <br><br> \"Deep Generative Classifiers for Thoracic Disease Diagnosis with Chest X-ray Images\" has done. <br>\n",
    "link: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6651749/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Import necessary modules\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "plt.rcParams['figure.figsize'] = [20, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the path to here\n",
    "\n",
    "Make sure the setup the paths properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\juan.pablo\\Documents\\temporal_school_rel\\CS231N-Final-Proj\\notebooks\n"
     ]
    }
   ],
   "source": [
    "#Path to assign tests (copy path directly)\n",
    "notebooks_path = os.getcwd() # OR MAYBE has to be set manually depending your computer\n",
    "\n",
    "#Set the path to this working directory\n",
    "os.chdir(notebooks_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "import sys\n",
    "#Append the path the src folder\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, \"src\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary module for downloading\n",
    "\n",
    "Note for this: EVERYTIME There is a change inside the download <br>\n",
    "the changes inside the file would only be shown if the jupyter kernel is restarted. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from utils import CXReader, DfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path\n",
    "df_path = os.path.join(notebooks_path, os.pardir, \"data\")\n",
    "data_path = os.path.join(df_path, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dataframes of the data\n",
    "First, lets obtain the dataframes for the data and check that all metadata <br>\n",
    "information has been set up properly. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: miccai2023_nih-cxr-lt_labels_test.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_train.csv has been retrieved\n",
      "The file: miccai2023_nih-cxr-lt_labels_val.csv has been retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a dataframe compiler\n",
    "df_compiler = DfReader()\n",
    "\n",
    "#set the path and retrieve the dataframes\n",
    "df_compiler.set_folder_path(df_path)\n",
    "\n",
    "#Get the dataframe holder and names\n",
    "dfs_holder, dfs_names = df_compiler.get_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "It can iterate through all batches\n"
     ]
    }
   ],
   "source": [
    "# Get the device if cuda or not\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define a transformations for the VGGnet16 (requires a 224,224)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.CenterCrop((224, 224)),  # Center crop to 224x224\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "#Create datasets and dataloaders\n",
    "test_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[0], transform=transform, device=device)\n",
    "train_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[1], transform=transform,device=device)\n",
    "val_dataset = CXReader(data_path=data_path, dataframe=dfs_holder[2], transform=transform, device=device)\n",
    "\n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#With batch size of 16, and shuffle true, and num workers = 4\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print an image and see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.0039, 0.0039,  ..., 0.0000, 0.0039, 0.0039],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0078, 0.0118, 0.0118],\n",
      "         [0.0275, 0.0275, 0.0275,  ..., 0.0196, 0.0196, 0.0196],\n",
      "         ...,\n",
      "         [0.1843, 0.2157, 0.2431,  ..., 0.5922, 0.5686, 0.5882],\n",
      "         [0.1804, 0.2118, 0.2431,  ..., 0.5843, 0.5647, 0.5961],\n",
      "         [0.1765, 0.2118, 0.2431,  ..., 0.5804, 0.5647, 0.6039]],\n",
      "\n",
      "        [[0.0039, 0.0039, 0.0039,  ..., 0.0000, 0.0039, 0.0039],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0078, 0.0118, 0.0118],\n",
      "         [0.0275, 0.0275, 0.0275,  ..., 0.0196, 0.0196, 0.0196],\n",
      "         ...,\n",
      "         [0.1843, 0.2157, 0.2431,  ..., 0.5922, 0.5686, 0.5882],\n",
      "         [0.1804, 0.2118, 0.2431,  ..., 0.5843, 0.5647, 0.5961],\n",
      "         [0.1765, 0.2118, 0.2431,  ..., 0.5804, 0.5647, 0.6039]],\n",
      "\n",
      "        [[0.0039, 0.0039, 0.0039,  ..., 0.0000, 0.0039, 0.0039],\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0078, 0.0118, 0.0118],\n",
      "         [0.0275, 0.0275, 0.0275,  ..., 0.0196, 0.0196, 0.0196],\n",
      "         ...,\n",
      "         [0.1843, 0.2157, 0.2431,  ..., 0.5922, 0.5686, 0.5882],\n",
      "         [0.1804, 0.2118, 0.2431,  ..., 0.5843, 0.5647, 0.5961],\n",
      "         [0.1765, 0.2118, 0.2431,  ..., 0.5804, 0.5647, 0.6039]]])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(samp3_image)\n",
    "print(samp3_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the vgg16_model features and set to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0370, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.5853, 1.1749,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.2216, 0.8938,  ..., 0.1804, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.8993, 0.7857, 0.6817,  ..., 0.3933, 0.7594, 1.0564],\n",
      "         [0.0000, 0.3005, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.3355, 0.3787,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.6487, 0.5645, 0.0000,  ..., 0.7035, 0.8460, 0.4126],\n",
      "         [1.0609, 0.6459, 0.0000,  ..., 0.6224, 0.7342, 0.1244],\n",
      "         [0.0000, 0.7672, 0.5803,  ..., 0.7205, 0.5931, 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0982],\n",
      "         [1.0025, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.6759, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [1.3725, 0.5892, 0.0000,  ..., 3.5520, 1.6942, 1.7013],\n",
      "         [2.4826, 2.3061, 0.0000,  ..., 3.4900, 4.2539, 4.4908],\n",
      "         [1.8253, 2.4452, 0.4153,  ..., 1.8031, 3.7034, 3.8943]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         ...,\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.4974, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>)\n",
      "torch.Size([512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class VGGEncoder(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGGEncoder, self).__init__()\n",
    "\n",
    "        # Load pre-trained VGG16 model\n",
    "        vgg16_model = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        # Use only the features part and remove the classifier\n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        # Set to evaluation mode if not fine-tuning\n",
    "        if not pretrained:\n",
    "            self.features.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Example usage\n",
    "# For fine-tuning all features spaces from vgg16_model\n",
    "#encoder_finetune = VGGEncoder(pretrained=False)\n",
    "#encoder_finetune.train()\n",
    "\n",
    "# For inference\n",
    "encoder_inference = VGGEncoder(pretrained=True)\n",
    "encoder_inference.eval()\n",
    "\n",
    "#Print output and output features\n",
    "output_features = encoder_inference(samp3_image)\n",
    "print(output_features)\n",
    "print(output_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have seen the VGG16 layer.\n",
    "Lets implement the solution where we flat the feature vector to a 1x1xD vector. <br>\n",
    "just as how the paper does it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 1, 1])\n",
      "tensor([[[[2.9033]],\n",
      "\n",
      "         [[1.9729]],\n",
      "\n",
      "         [[2.6344]],\n",
      "\n",
      "         [[2.1812]],\n",
      "\n",
      "         [[1.8654]],\n",
      "\n",
      "         [[1.6665]],\n",
      "\n",
      "         [[1.6841]],\n",
      "\n",
      "         [[2.8485]],\n",
      "\n",
      "         [[1.9693]],\n",
      "\n",
      "         [[1.9428]],\n",
      "\n",
      "         [[1.9075]],\n",
      "\n",
      "         [[2.2622]],\n",
      "\n",
      "         [[2.4120]],\n",
      "\n",
      "         [[3.1667]],\n",
      "\n",
      "         [[2.6403]],\n",
      "\n",
      "         [[2.6508]],\n",
      "\n",
      "         [[1.8631]],\n",
      "\n",
      "         [[3.6698]],\n",
      "\n",
      "         [[1.8254]],\n",
      "\n",
      "         [[2.6148]],\n",
      "\n",
      "         [[2.9299]],\n",
      "\n",
      "         [[1.8592]],\n",
      "\n",
      "         [[2.3962]],\n",
      "\n",
      "         [[2.4487]],\n",
      "\n",
      "         [[2.2068]],\n",
      "\n",
      "         [[2.4195]],\n",
      "\n",
      "         [[2.2245]],\n",
      "\n",
      "         [[1.9411]],\n",
      "\n",
      "         [[2.2651]],\n",
      "\n",
      "         [[1.8397]],\n",
      "\n",
      "         [[2.1052]],\n",
      "\n",
      "         [[2.2936]],\n",
      "\n",
      "         [[2.3748]],\n",
      "\n",
      "         [[3.1480]],\n",
      "\n",
      "         [[1.6557]],\n",
      "\n",
      "         [[2.6824]],\n",
      "\n",
      "         [[2.5306]],\n",
      "\n",
      "         [[0.9486]],\n",
      "\n",
      "         [[2.3012]],\n",
      "\n",
      "         [[2.8174]],\n",
      "\n",
      "         [[1.8243]],\n",
      "\n",
      "         [[3.5568]],\n",
      "\n",
      "         [[2.0093]],\n",
      "\n",
      "         [[2.7931]],\n",
      "\n",
      "         [[1.7238]],\n",
      "\n",
      "         [[2.0657]],\n",
      "\n",
      "         [[2.3487]],\n",
      "\n",
      "         [[2.0801]],\n",
      "\n",
      "         [[2.3172]],\n",
      "\n",
      "         [[2.9113]],\n",
      "\n",
      "         [[2.3139]],\n",
      "\n",
      "         [[1.5214]],\n",
      "\n",
      "         [[1.9967]],\n",
      "\n",
      "         [[2.1830]],\n",
      "\n",
      "         [[2.5771]],\n",
      "\n",
      "         [[2.6195]],\n",
      "\n",
      "         [[2.9955]],\n",
      "\n",
      "         [[1.9169]],\n",
      "\n",
      "         [[2.4057]],\n",
      "\n",
      "         [[2.8261]],\n",
      "\n",
      "         [[2.3284]],\n",
      "\n",
      "         [[2.7570]],\n",
      "\n",
      "         [[2.4815]],\n",
      "\n",
      "         [[2.1276]],\n",
      "\n",
      "         [[1.7918]],\n",
      "\n",
      "         [[1.8508]],\n",
      "\n",
      "         [[1.8269]],\n",
      "\n",
      "         [[1.6982]],\n",
      "\n",
      "         [[2.4019]],\n",
      "\n",
      "         [[2.4365]],\n",
      "\n",
      "         [[2.9921]],\n",
      "\n",
      "         [[2.8319]],\n",
      "\n",
      "         [[1.8556]],\n",
      "\n",
      "         [[2.1252]],\n",
      "\n",
      "         [[2.7341]],\n",
      "\n",
      "         [[2.1750]],\n",
      "\n",
      "         [[2.5172]],\n",
      "\n",
      "         [[2.1726]],\n",
      "\n",
      "         [[3.3635]],\n",
      "\n",
      "         [[2.5082]],\n",
      "\n",
      "         [[2.0530]],\n",
      "\n",
      "         [[2.2489]],\n",
      "\n",
      "         [[2.3353]],\n",
      "\n",
      "         [[2.5768]],\n",
      "\n",
      "         [[2.4514]],\n",
      "\n",
      "         [[2.7024]],\n",
      "\n",
      "         [[2.4667]],\n",
      "\n",
      "         [[2.8020]],\n",
      "\n",
      "         [[2.0866]],\n",
      "\n",
      "         [[2.7871]],\n",
      "\n",
      "         [[1.5355]],\n",
      "\n",
      "         [[2.1929]],\n",
      "\n",
      "         [[2.2243]],\n",
      "\n",
      "         [[2.2396]],\n",
      "\n",
      "         [[1.8739]],\n",
      "\n",
      "         [[1.9923]],\n",
      "\n",
      "         [[2.1571]],\n",
      "\n",
      "         [[2.2299]],\n",
      "\n",
      "         [[2.8541]],\n",
      "\n",
      "         [[2.1616]],\n",
      "\n",
      "         [[2.0357]],\n",
      "\n",
      "         [[2.5463]],\n",
      "\n",
      "         [[1.7565]],\n",
      "\n",
      "         [[2.0410]],\n",
      "\n",
      "         [[2.5161]],\n",
      "\n",
      "         [[1.8047]],\n",
      "\n",
      "         [[1.7967]],\n",
      "\n",
      "         [[2.7161]],\n",
      "\n",
      "         [[2.0927]],\n",
      "\n",
      "         [[2.2140]],\n",
      "\n",
      "         [[1.7726]],\n",
      "\n",
      "         [[1.9192]],\n",
      "\n",
      "         [[1.6655]],\n",
      "\n",
      "         [[2.3643]],\n",
      "\n",
      "         [[2.0162]],\n",
      "\n",
      "         [[2.1181]],\n",
      "\n",
      "         [[2.6336]],\n",
      "\n",
      "         [[1.7988]],\n",
      "\n",
      "         [[2.9848]],\n",
      "\n",
      "         [[1.8282]],\n",
      "\n",
      "         [[2.7700]],\n",
      "\n",
      "         [[2.2024]],\n",
      "\n",
      "         [[1.6151]],\n",
      "\n",
      "         [[1.9665]],\n",
      "\n",
      "         [[2.3337]],\n",
      "\n",
      "         [[1.3922]],\n",
      "\n",
      "         [[2.1741]],\n",
      "\n",
      "         [[2.9361]],\n",
      "\n",
      "         [[2.3565]],\n",
      "\n",
      "         [[1.6775]],\n",
      "\n",
      "         [[1.6026]],\n",
      "\n",
      "         [[2.0680]],\n",
      "\n",
      "         [[3.0818]],\n",
      "\n",
      "         [[2.4010]],\n",
      "\n",
      "         [[1.9257]],\n",
      "\n",
      "         [[2.5722]],\n",
      "\n",
      "         [[2.6178]],\n",
      "\n",
      "         [[2.6179]],\n",
      "\n",
      "         [[1.8598]],\n",
      "\n",
      "         [[2.0967]],\n",
      "\n",
      "         [[2.1164]],\n",
      "\n",
      "         [[2.5262]],\n",
      "\n",
      "         [[2.2094]],\n",
      "\n",
      "         [[1.8750]],\n",
      "\n",
      "         [[2.4688]],\n",
      "\n",
      "         [[2.1286]],\n",
      "\n",
      "         [[2.2252]],\n",
      "\n",
      "         [[1.7032]],\n",
      "\n",
      "         [[1.8815]],\n",
      "\n",
      "         [[1.8566]],\n",
      "\n",
      "         [[2.4184]],\n",
      "\n",
      "         [[2.3177]],\n",
      "\n",
      "         [[2.6558]],\n",
      "\n",
      "         [[2.2577]],\n",
      "\n",
      "         [[2.8488]],\n",
      "\n",
      "         [[1.8185]],\n",
      "\n",
      "         [[2.0783]],\n",
      "\n",
      "         [[2.1556]],\n",
      "\n",
      "         [[2.3049]],\n",
      "\n",
      "         [[2.0019]],\n",
      "\n",
      "         [[1.9259]],\n",
      "\n",
      "         [[2.9137]],\n",
      "\n",
      "         [[2.6578]],\n",
      "\n",
      "         [[3.2428]],\n",
      "\n",
      "         [[2.4692]],\n",
      "\n",
      "         [[2.0126]],\n",
      "\n",
      "         [[2.3728]],\n",
      "\n",
      "         [[2.3774]],\n",
      "\n",
      "         [[2.1306]],\n",
      "\n",
      "         [[1.7834]],\n",
      "\n",
      "         [[1.8060]],\n",
      "\n",
      "         [[2.4233]],\n",
      "\n",
      "         [[1.4687]],\n",
      "\n",
      "         [[1.5582]],\n",
      "\n",
      "         [[1.7987]],\n",
      "\n",
      "         [[2.7011]],\n",
      "\n",
      "         [[2.1117]],\n",
      "\n",
      "         [[2.3316]],\n",
      "\n",
      "         [[1.5651]],\n",
      "\n",
      "         [[2.3314]],\n",
      "\n",
      "         [[2.3307]],\n",
      "\n",
      "         [[2.6421]],\n",
      "\n",
      "         [[2.0998]],\n",
      "\n",
      "         [[2.0667]],\n",
      "\n",
      "         [[2.4595]],\n",
      "\n",
      "         [[2.5051]],\n",
      "\n",
      "         [[2.9946]],\n",
      "\n",
      "         [[1.8824]],\n",
      "\n",
      "         [[2.5713]],\n",
      "\n",
      "         [[2.6866]],\n",
      "\n",
      "         [[2.0648]],\n",
      "\n",
      "         [[1.6712]],\n",
      "\n",
      "         [[1.9099]],\n",
      "\n",
      "         [[2.3302]],\n",
      "\n",
      "         [[2.8585]],\n",
      "\n",
      "         [[1.8793]],\n",
      "\n",
      "         [[1.8439]],\n",
      "\n",
      "         [[1.6776]],\n",
      "\n",
      "         [[3.1625]],\n",
      "\n",
      "         [[2.5457]],\n",
      "\n",
      "         [[2.9560]],\n",
      "\n",
      "         [[2.4600]],\n",
      "\n",
      "         [[2.0174]],\n",
      "\n",
      "         [[2.0250]],\n",
      "\n",
      "         [[1.9526]],\n",
      "\n",
      "         [[2.8857]],\n",
      "\n",
      "         [[1.7784]],\n",
      "\n",
      "         [[2.6419]],\n",
      "\n",
      "         [[2.3118]],\n",
      "\n",
      "         [[2.2677]],\n",
      "\n",
      "         [[1.8230]],\n",
      "\n",
      "         [[2.8889]],\n",
      "\n",
      "         [[2.1794]],\n",
      "\n",
      "         [[2.3587]],\n",
      "\n",
      "         [[2.2548]],\n",
      "\n",
      "         [[2.4223]],\n",
      "\n",
      "         [[2.4791]],\n",
      "\n",
      "         [[2.2182]],\n",
      "\n",
      "         [[1.6543]],\n",
      "\n",
      "         [[2.2636]],\n",
      "\n",
      "         [[2.3083]],\n",
      "\n",
      "         [[1.9834]],\n",
      "\n",
      "         [[2.4675]],\n",
      "\n",
      "         [[1.8959]],\n",
      "\n",
      "         [[2.7332]],\n",
      "\n",
      "         [[2.7585]],\n",
      "\n",
      "         [[1.7487]],\n",
      "\n",
      "         [[2.0382]],\n",
      "\n",
      "         [[2.3102]],\n",
      "\n",
      "         [[1.8419]],\n",
      "\n",
      "         [[2.0234]],\n",
      "\n",
      "         [[1.2746]],\n",
      "\n",
      "         [[1.7000]],\n",
      "\n",
      "         [[2.2742]],\n",
      "\n",
      "         [[3.7695]],\n",
      "\n",
      "         [[1.8523]],\n",
      "\n",
      "         [[2.1671]],\n",
      "\n",
      "         [[1.7014]],\n",
      "\n",
      "         [[1.9111]],\n",
      "\n",
      "         [[2.0744]],\n",
      "\n",
      "         [[2.3837]],\n",
      "\n",
      "         [[2.2793]],\n",
      "\n",
      "         [[2.1446]],\n",
      "\n",
      "         [[1.7382]],\n",
      "\n",
      "         [[4.0213]],\n",
      "\n",
      "         [[1.7472]],\n",
      "\n",
      "         [[3.3258]],\n",
      "\n",
      "         [[2.0261]],\n",
      "\n",
      "         [[1.5524]],\n",
      "\n",
      "         [[2.1296]],\n",
      "\n",
      "         [[2.0060]],\n",
      "\n",
      "         [[2.6808]],\n",
      "\n",
      "         [[2.6249]],\n",
      "\n",
      "         [[2.8394]],\n",
      "\n",
      "         [[2.0371]],\n",
      "\n",
      "         [[2.3518]],\n",
      "\n",
      "         [[2.0202]],\n",
      "\n",
      "         [[2.3985]],\n",
      "\n",
      "         [[2.1879]],\n",
      "\n",
      "         [[2.3003]],\n",
      "\n",
      "         [[1.8220]],\n",
      "\n",
      "         [[3.5492]],\n",
      "\n",
      "         [[2.5937]],\n",
      "\n",
      "         [[2.1103]],\n",
      "\n",
      "         [[2.0227]],\n",
      "\n",
      "         [[2.5056]],\n",
      "\n",
      "         [[2.6239]],\n",
      "\n",
      "         [[1.7198]],\n",
      "\n",
      "         [[2.0638]],\n",
      "\n",
      "         [[1.9771]],\n",
      "\n",
      "         [[2.1322]],\n",
      "\n",
      "         [[1.7891]],\n",
      "\n",
      "         [[2.3004]],\n",
      "\n",
      "         [[1.9036]],\n",
      "\n",
      "         [[2.0648]],\n",
      "\n",
      "         [[2.2000]],\n",
      "\n",
      "         [[1.9092]],\n",
      "\n",
      "         [[2.7219]],\n",
      "\n",
      "         [[2.0533]],\n",
      "\n",
      "         [[2.3145]],\n",
      "\n",
      "         [[1.3756]],\n",
      "\n",
      "         [[2.4676]],\n",
      "\n",
      "         [[2.1220]],\n",
      "\n",
      "         [[2.8190]],\n",
      "\n",
      "         [[1.9702]],\n",
      "\n",
      "         [[2.5065]],\n",
      "\n",
      "         [[1.9954]],\n",
      "\n",
      "         [[2.5862]],\n",
      "\n",
      "         [[1.8341]],\n",
      "\n",
      "         [[1.6667]],\n",
      "\n",
      "         [[3.3354]],\n",
      "\n",
      "         [[2.1841]],\n",
      "\n",
      "         [[1.7253]],\n",
      "\n",
      "         [[2.5843]],\n",
      "\n",
      "         [[2.1384]],\n",
      "\n",
      "         [[2.0968]],\n",
      "\n",
      "         [[2.5144]],\n",
      "\n",
      "         [[3.0036]],\n",
      "\n",
      "         [[3.1166]],\n",
      "\n",
      "         [[2.4371]]],\n",
      "\n",
      "\n",
      "        [[[2.0422]],\n",
      "\n",
      "         [[2.0794]],\n",
      "\n",
      "         [[2.5727]],\n",
      "\n",
      "         [[1.8183]],\n",
      "\n",
      "         [[1.7626]],\n",
      "\n",
      "         [[3.2665]],\n",
      "\n",
      "         [[2.6825]],\n",
      "\n",
      "         [[1.8964]],\n",
      "\n",
      "         [[3.4086]],\n",
      "\n",
      "         [[2.0979]],\n",
      "\n",
      "         [[1.5507]],\n",
      "\n",
      "         [[1.7232]],\n",
      "\n",
      "         [[1.6742]],\n",
      "\n",
      "         [[2.4911]],\n",
      "\n",
      "         [[1.9820]],\n",
      "\n",
      "         [[2.6699]],\n",
      "\n",
      "         [[2.6728]],\n",
      "\n",
      "         [[2.3216]],\n",
      "\n",
      "         [[2.5228]],\n",
      "\n",
      "         [[2.1829]],\n",
      "\n",
      "         [[3.2922]],\n",
      "\n",
      "         [[2.6978]],\n",
      "\n",
      "         [[3.1272]],\n",
      "\n",
      "         [[2.2059]],\n",
      "\n",
      "         [[3.1667]],\n",
      "\n",
      "         [[2.2197]],\n",
      "\n",
      "         [[1.8826]],\n",
      "\n",
      "         [[1.4116]],\n",
      "\n",
      "         [[2.1419]],\n",
      "\n",
      "         [[2.4174]],\n",
      "\n",
      "         [[1.9065]],\n",
      "\n",
      "         [[1.7764]],\n",
      "\n",
      "         [[1.8927]],\n",
      "\n",
      "         [[1.9985]],\n",
      "\n",
      "         [[2.6306]],\n",
      "\n",
      "         [[1.8143]],\n",
      "\n",
      "         [[2.2472]],\n",
      "\n",
      "         [[2.7508]],\n",
      "\n",
      "         [[1.8929]],\n",
      "\n",
      "         [[2.6762]],\n",
      "\n",
      "         [[2.3534]],\n",
      "\n",
      "         [[2.7734]],\n",
      "\n",
      "         [[2.5073]],\n",
      "\n",
      "         [[1.7421]],\n",
      "\n",
      "         [[2.4811]],\n",
      "\n",
      "         [[2.2677]],\n",
      "\n",
      "         [[2.4689]],\n",
      "\n",
      "         [[2.3790]],\n",
      "\n",
      "         [[2.1960]],\n",
      "\n",
      "         [[2.3494]],\n",
      "\n",
      "         [[2.1267]],\n",
      "\n",
      "         [[2.1040]],\n",
      "\n",
      "         [[2.3978]],\n",
      "\n",
      "         [[1.7456]],\n",
      "\n",
      "         [[3.4832]],\n",
      "\n",
      "         [[2.0202]],\n",
      "\n",
      "         [[1.9550]],\n",
      "\n",
      "         [[2.3207]],\n",
      "\n",
      "         [[2.7308]],\n",
      "\n",
      "         [[2.2998]],\n",
      "\n",
      "         [[1.9659]],\n",
      "\n",
      "         [[1.9775]],\n",
      "\n",
      "         [[1.8179]],\n",
      "\n",
      "         [[2.0319]],\n",
      "\n",
      "         [[3.7921]],\n",
      "\n",
      "         [[1.9758]],\n",
      "\n",
      "         [[1.8484]],\n",
      "\n",
      "         [[3.6636]],\n",
      "\n",
      "         [[1.9876]],\n",
      "\n",
      "         [[2.4201]],\n",
      "\n",
      "         [[2.2057]],\n",
      "\n",
      "         [[1.9360]],\n",
      "\n",
      "         [[2.8032]],\n",
      "\n",
      "         [[2.7258]],\n",
      "\n",
      "         [[2.4126]],\n",
      "\n",
      "         [[1.8552]],\n",
      "\n",
      "         [[1.9327]],\n",
      "\n",
      "         [[1.6515]],\n",
      "\n",
      "         [[1.7091]],\n",
      "\n",
      "         [[2.0418]],\n",
      "\n",
      "         [[2.0732]],\n",
      "\n",
      "         [[2.1250]],\n",
      "\n",
      "         [[2.4404]],\n",
      "\n",
      "         [[2.9232]],\n",
      "\n",
      "         [[2.3974]],\n",
      "\n",
      "         [[2.1473]],\n",
      "\n",
      "         [[1.8810]],\n",
      "\n",
      "         [[2.5360]],\n",
      "\n",
      "         [[2.6140]],\n",
      "\n",
      "         [[1.6338]],\n",
      "\n",
      "         [[2.0430]],\n",
      "\n",
      "         [[1.9623]],\n",
      "\n",
      "         [[1.7953]],\n",
      "\n",
      "         [[3.0027]],\n",
      "\n",
      "         [[1.9424]],\n",
      "\n",
      "         [[2.0278]],\n",
      "\n",
      "         [[2.1789]],\n",
      "\n",
      "         [[2.5844]],\n",
      "\n",
      "         [[1.8916]],\n",
      "\n",
      "         [[2.2866]],\n",
      "\n",
      "         [[2.8832]],\n",
      "\n",
      "         [[1.6832]],\n",
      "\n",
      "         [[2.3906]],\n",
      "\n",
      "         [[2.5097]],\n",
      "\n",
      "         [[2.6388]],\n",
      "\n",
      "         [[2.6345]],\n",
      "\n",
      "         [[2.1343]],\n",
      "\n",
      "         [[2.0441]],\n",
      "\n",
      "         [[1.9812]],\n",
      "\n",
      "         [[3.1706]],\n",
      "\n",
      "         [[1.7582]],\n",
      "\n",
      "         [[2.8467]],\n",
      "\n",
      "         [[2.4937]],\n",
      "\n",
      "         [[2.2292]],\n",
      "\n",
      "         [[2.5995]],\n",
      "\n",
      "         [[1.8941]],\n",
      "\n",
      "         [[2.1232]],\n",
      "\n",
      "         [[2.3271]],\n",
      "\n",
      "         [[2.3491]],\n",
      "\n",
      "         [[2.4793]],\n",
      "\n",
      "         [[2.0502]],\n",
      "\n",
      "         [[2.0969]],\n",
      "\n",
      "         [[2.8327]],\n",
      "\n",
      "         [[1.7777]],\n",
      "\n",
      "         [[1.7706]],\n",
      "\n",
      "         [[2.6264]],\n",
      "\n",
      "         [[1.5013]],\n",
      "\n",
      "         [[2.2241]],\n",
      "\n",
      "         [[2.6072]],\n",
      "\n",
      "         [[2.7622]],\n",
      "\n",
      "         [[2.5653]],\n",
      "\n",
      "         [[2.0607]],\n",
      "\n",
      "         [[2.2012]],\n",
      "\n",
      "         [[2.4157]],\n",
      "\n",
      "         [[2.2576]],\n",
      "\n",
      "         [[1.9544]],\n",
      "\n",
      "         [[1.9505]],\n",
      "\n",
      "         [[2.0410]],\n",
      "\n",
      "         [[2.0718]],\n",
      "\n",
      "         [[1.8604]],\n",
      "\n",
      "         [[2.6335]],\n",
      "\n",
      "         [[1.4768]],\n",
      "\n",
      "         [[2.1209]],\n",
      "\n",
      "         [[2.3581]],\n",
      "\n",
      "         [[1.6398]],\n",
      "\n",
      "         [[2.5623]],\n",
      "\n",
      "         [[2.5879]],\n",
      "\n",
      "         [[2.5772]],\n",
      "\n",
      "         [[2.1426]],\n",
      "\n",
      "         [[2.5153]],\n",
      "\n",
      "         [[2.5706]],\n",
      "\n",
      "         [[2.2414]],\n",
      "\n",
      "         [[2.1738]],\n",
      "\n",
      "         [[2.0156]],\n",
      "\n",
      "         [[2.3371]],\n",
      "\n",
      "         [[2.7867]],\n",
      "\n",
      "         [[2.6545]],\n",
      "\n",
      "         [[2.1629]],\n",
      "\n",
      "         [[2.4501]],\n",
      "\n",
      "         [[2.4888]],\n",
      "\n",
      "         [[2.0095]],\n",
      "\n",
      "         [[2.0740]],\n",
      "\n",
      "         [[1.7175]],\n",
      "\n",
      "         [[2.2153]],\n",
      "\n",
      "         [[2.5071]],\n",
      "\n",
      "         [[2.6143]],\n",
      "\n",
      "         [[2.7012]],\n",
      "\n",
      "         [[2.5231]],\n",
      "\n",
      "         [[3.1096]],\n",
      "\n",
      "         [[2.2134]],\n",
      "\n",
      "         [[2.4890]],\n",
      "\n",
      "         [[2.5033]],\n",
      "\n",
      "         [[1.6390]],\n",
      "\n",
      "         [[2.7720]],\n",
      "\n",
      "         [[2.3737]],\n",
      "\n",
      "         [[2.4436]],\n",
      "\n",
      "         [[2.3388]],\n",
      "\n",
      "         [[2.0632]],\n",
      "\n",
      "         [[1.9065]],\n",
      "\n",
      "         [[2.0186]],\n",
      "\n",
      "         [[2.0755]],\n",
      "\n",
      "         [[2.5754]],\n",
      "\n",
      "         [[1.7609]],\n",
      "\n",
      "         [[2.2151]],\n",
      "\n",
      "         [[1.6184]],\n",
      "\n",
      "         [[1.3133]],\n",
      "\n",
      "         [[2.6058]],\n",
      "\n",
      "         [[3.2399]],\n",
      "\n",
      "         [[2.5843]],\n",
      "\n",
      "         [[2.4591]],\n",
      "\n",
      "         [[2.1138]],\n",
      "\n",
      "         [[2.9240]],\n",
      "\n",
      "         [[1.5284]],\n",
      "\n",
      "         [[2.5604]],\n",
      "\n",
      "         [[2.7962]],\n",
      "\n",
      "         [[2.5268]],\n",
      "\n",
      "         [[2.4241]],\n",
      "\n",
      "         [[2.1308]],\n",
      "\n",
      "         [[2.5443]],\n",
      "\n",
      "         [[1.7874]],\n",
      "\n",
      "         [[2.2523]],\n",
      "\n",
      "         [[2.4701]],\n",
      "\n",
      "         [[2.9146]],\n",
      "\n",
      "         [[2.2088]],\n",
      "\n",
      "         [[1.7677]],\n",
      "\n",
      "         [[2.0677]],\n",
      "\n",
      "         [[1.8111]],\n",
      "\n",
      "         [[2.5220]],\n",
      "\n",
      "         [[1.8355]],\n",
      "\n",
      "         [[2.3105]],\n",
      "\n",
      "         [[2.3585]],\n",
      "\n",
      "         [[2.8166]],\n",
      "\n",
      "         [[1.7964]],\n",
      "\n",
      "         [[2.9631]],\n",
      "\n",
      "         [[2.5569]],\n",
      "\n",
      "         [[2.1791]],\n",
      "\n",
      "         [[2.4588]],\n",
      "\n",
      "         [[2.3886]],\n",
      "\n",
      "         [[2.4676]],\n",
      "\n",
      "         [[3.4799]],\n",
      "\n",
      "         [[2.7436]],\n",
      "\n",
      "         [[2.4254]],\n",
      "\n",
      "         [[3.2939]],\n",
      "\n",
      "         [[2.3943]],\n",
      "\n",
      "         [[2.0641]],\n",
      "\n",
      "         [[1.5860]],\n",
      "\n",
      "         [[1.8499]],\n",
      "\n",
      "         [[2.4786]],\n",
      "\n",
      "         [[2.1360]],\n",
      "\n",
      "         [[2.2493]],\n",
      "\n",
      "         [[2.3897]],\n",
      "\n",
      "         [[2.3975]],\n",
      "\n",
      "         [[1.6507]],\n",
      "\n",
      "         [[2.0894]],\n",
      "\n",
      "         [[2.4904]],\n",
      "\n",
      "         [[2.3493]],\n",
      "\n",
      "         [[1.9872]],\n",
      "\n",
      "         [[2.0148]],\n",
      "\n",
      "         [[2.5096]],\n",
      "\n",
      "         [[2.9550]],\n",
      "\n",
      "         [[2.0400]],\n",
      "\n",
      "         [[1.8791]],\n",
      "\n",
      "         [[2.2905]],\n",
      "\n",
      "         [[2.9339]],\n",
      "\n",
      "         [[2.2248]],\n",
      "\n",
      "         [[2.6289]],\n",
      "\n",
      "         [[2.2229]],\n",
      "\n",
      "         [[1.8722]],\n",
      "\n",
      "         [[2.3074]],\n",
      "\n",
      "         [[1.7371]],\n",
      "\n",
      "         [[2.1322]],\n",
      "\n",
      "         [[2.6185]],\n",
      "\n",
      "         [[1.8095]],\n",
      "\n",
      "         [[1.5405]],\n",
      "\n",
      "         [[2.0961]],\n",
      "\n",
      "         [[1.7930]],\n",
      "\n",
      "         [[1.8216]],\n",
      "\n",
      "         [[1.9240]],\n",
      "\n",
      "         [[2.3174]],\n",
      "\n",
      "         [[2.1844]],\n",
      "\n",
      "         [[2.6612]],\n",
      "\n",
      "         [[2.2256]],\n",
      "\n",
      "         [[1.8699]],\n",
      "\n",
      "         [[2.4085]],\n",
      "\n",
      "         [[2.7462]],\n",
      "\n",
      "         [[1.9826]],\n",
      "\n",
      "         [[1.9005]],\n",
      "\n",
      "         [[2.9581]],\n",
      "\n",
      "         [[1.8350]],\n",
      "\n",
      "         [[2.2213]],\n",
      "\n",
      "         [[1.6848]],\n",
      "\n",
      "         [[2.1543]],\n",
      "\n",
      "         [[2.0265]],\n",
      "\n",
      "         [[2.8722]],\n",
      "\n",
      "         [[2.2854]],\n",
      "\n",
      "         [[2.0876]],\n",
      "\n",
      "         [[1.8322]],\n",
      "\n",
      "         [[2.0334]],\n",
      "\n",
      "         [[2.0759]],\n",
      "\n",
      "         [[2.2755]],\n",
      "\n",
      "         [[2.0812]],\n",
      "\n",
      "         [[2.0642]],\n",
      "\n",
      "         [[1.9347]],\n",
      "\n",
      "         [[1.4523]],\n",
      "\n",
      "         [[2.0460]],\n",
      "\n",
      "         [[1.5093]],\n",
      "\n",
      "         [[1.6862]],\n",
      "\n",
      "         [[1.6031]],\n",
      "\n",
      "         [[3.0442]],\n",
      "\n",
      "         [[2.1054]],\n",
      "\n",
      "         [[2.7698]],\n",
      "\n",
      "         [[1.9564]],\n",
      "\n",
      "         [[2.5117]],\n",
      "\n",
      "         [[1.5659]],\n",
      "\n",
      "         [[1.7776]],\n",
      "\n",
      "         [[2.4880]],\n",
      "\n",
      "         [[2.7001]],\n",
      "\n",
      "         [[2.2237]],\n",
      "\n",
      "         [[2.5524]],\n",
      "\n",
      "         [[2.5068]]],\n",
      "\n",
      "\n",
      "        [[[2.8528]],\n",
      "\n",
      "         [[3.0404]],\n",
      "\n",
      "         [[1.7402]],\n",
      "\n",
      "         [[2.1536]],\n",
      "\n",
      "         [[1.7409]],\n",
      "\n",
      "         [[1.3318]],\n",
      "\n",
      "         [[1.7564]],\n",
      "\n",
      "         [[2.5116]],\n",
      "\n",
      "         [[2.2534]],\n",
      "\n",
      "         [[1.8219]],\n",
      "\n",
      "         [[2.2016]],\n",
      "\n",
      "         [[1.7747]],\n",
      "\n",
      "         [[2.2341]],\n",
      "\n",
      "         [[1.4297]],\n",
      "\n",
      "         [[2.2602]],\n",
      "\n",
      "         [[1.6053]],\n",
      "\n",
      "         [[2.8395]],\n",
      "\n",
      "         [[2.3085]],\n",
      "\n",
      "         [[1.9448]],\n",
      "\n",
      "         [[1.4707]],\n",
      "\n",
      "         [[1.5563]],\n",
      "\n",
      "         [[1.5457]],\n",
      "\n",
      "         [[3.0545]],\n",
      "\n",
      "         [[1.5595]],\n",
      "\n",
      "         [[1.9915]],\n",
      "\n",
      "         [[2.1402]],\n",
      "\n",
      "         [[2.0238]],\n",
      "\n",
      "         [[2.7554]],\n",
      "\n",
      "         [[1.9408]],\n",
      "\n",
      "         [[1.7167]],\n",
      "\n",
      "         [[2.3838]],\n",
      "\n",
      "         [[1.4772]],\n",
      "\n",
      "         [[1.8788]],\n",
      "\n",
      "         [[2.7402]],\n",
      "\n",
      "         [[2.9949]],\n",
      "\n",
      "         [[2.2339]],\n",
      "\n",
      "         [[2.2590]],\n",
      "\n",
      "         [[2.8981]],\n",
      "\n",
      "         [[1.7879]],\n",
      "\n",
      "         [[2.6363]],\n",
      "\n",
      "         [[2.3706]],\n",
      "\n",
      "         [[3.2523]],\n",
      "\n",
      "         [[2.0915]],\n",
      "\n",
      "         [[2.2277]],\n",
      "\n",
      "         [[2.7415]],\n",
      "\n",
      "         [[2.5342]],\n",
      "\n",
      "         [[1.8167]],\n",
      "\n",
      "         [[2.7032]],\n",
      "\n",
      "         [[2.7847]],\n",
      "\n",
      "         [[1.6886]],\n",
      "\n",
      "         [[1.5864]],\n",
      "\n",
      "         [[3.0283]],\n",
      "\n",
      "         [[2.1008]],\n",
      "\n",
      "         [[2.1941]],\n",
      "\n",
      "         [[2.2440]],\n",
      "\n",
      "         [[1.6318]],\n",
      "\n",
      "         [[2.1880]],\n",
      "\n",
      "         [[2.1093]],\n",
      "\n",
      "         [[1.9999]],\n",
      "\n",
      "         [[2.1427]],\n",
      "\n",
      "         [[2.5906]],\n",
      "\n",
      "         [[1.9964]],\n",
      "\n",
      "         [[2.5396]],\n",
      "\n",
      "         [[1.9902]],\n",
      "\n",
      "         [[2.3505]],\n",
      "\n",
      "         [[2.5434]],\n",
      "\n",
      "         [[2.9633]],\n",
      "\n",
      "         [[2.6244]],\n",
      "\n",
      "         [[2.8018]],\n",
      "\n",
      "         [[2.4700]],\n",
      "\n",
      "         [[1.7846]],\n",
      "\n",
      "         [[2.3504]],\n",
      "\n",
      "         [[2.1469]],\n",
      "\n",
      "         [[1.8090]],\n",
      "\n",
      "         [[2.3993]],\n",
      "\n",
      "         [[2.8792]],\n",
      "\n",
      "         [[2.0454]],\n",
      "\n",
      "         [[2.5657]],\n",
      "\n",
      "         [[1.9960]],\n",
      "\n",
      "         [[3.0736]],\n",
      "\n",
      "         [[2.1714]],\n",
      "\n",
      "         [[2.8388]],\n",
      "\n",
      "         [[1.8507]],\n",
      "\n",
      "         [[2.9553]],\n",
      "\n",
      "         [[2.6211]],\n",
      "\n",
      "         [[2.3342]],\n",
      "\n",
      "         [[2.1620]],\n",
      "\n",
      "         [[2.1160]],\n",
      "\n",
      "         [[1.6859]],\n",
      "\n",
      "         [[2.0232]],\n",
      "\n",
      "         [[2.1814]],\n",
      "\n",
      "         [[1.7913]],\n",
      "\n",
      "         [[1.4187]],\n",
      "\n",
      "         [[2.4512]],\n",
      "\n",
      "         [[2.6859]],\n",
      "\n",
      "         [[2.9034]],\n",
      "\n",
      "         [[2.2570]],\n",
      "\n",
      "         [[1.2932]],\n",
      "\n",
      "         [[2.2565]],\n",
      "\n",
      "         [[2.1620]],\n",
      "\n",
      "         [[2.0452]],\n",
      "\n",
      "         [[2.2387]],\n",
      "\n",
      "         [[1.5860]],\n",
      "\n",
      "         [[2.5528]],\n",
      "\n",
      "         [[2.8741]],\n",
      "\n",
      "         [[2.1184]],\n",
      "\n",
      "         [[2.9492]],\n",
      "\n",
      "         [[2.4638]],\n",
      "\n",
      "         [[2.5508]],\n",
      "\n",
      "         [[2.7580]],\n",
      "\n",
      "         [[2.2464]],\n",
      "\n",
      "         [[2.2162]],\n",
      "\n",
      "         [[2.4690]],\n",
      "\n",
      "         [[2.9866]],\n",
      "\n",
      "         [[2.0592]],\n",
      "\n",
      "         [[2.3211]],\n",
      "\n",
      "         [[2.4314]],\n",
      "\n",
      "         [[1.8570]],\n",
      "\n",
      "         [[2.6197]],\n",
      "\n",
      "         [[2.5723]],\n",
      "\n",
      "         [[1.8071]],\n",
      "\n",
      "         [[2.2539]],\n",
      "\n",
      "         [[2.4490]],\n",
      "\n",
      "         [[2.3682]],\n",
      "\n",
      "         [[2.0265]],\n",
      "\n",
      "         [[1.8258]],\n",
      "\n",
      "         [[2.3534]],\n",
      "\n",
      "         [[1.9762]],\n",
      "\n",
      "         [[1.6859]],\n",
      "\n",
      "         [[2.2632]],\n",
      "\n",
      "         [[3.6029]],\n",
      "\n",
      "         [[2.1716]],\n",
      "\n",
      "         [[2.3523]],\n",
      "\n",
      "         [[2.5667]],\n",
      "\n",
      "         [[1.8065]],\n",
      "\n",
      "         [[2.3866]],\n",
      "\n",
      "         [[1.8744]],\n",
      "\n",
      "         [[2.2372]],\n",
      "\n",
      "         [[2.0970]],\n",
      "\n",
      "         [[1.9980]],\n",
      "\n",
      "         [[3.0795]],\n",
      "\n",
      "         [[3.3291]],\n",
      "\n",
      "         [[1.4438]],\n",
      "\n",
      "         [[1.5562]],\n",
      "\n",
      "         [[2.3098]],\n",
      "\n",
      "         [[1.8502]],\n",
      "\n",
      "         [[1.6836]],\n",
      "\n",
      "         [[2.1755]],\n",
      "\n",
      "         [[1.9687]],\n",
      "\n",
      "         [[2.4010]],\n",
      "\n",
      "         [[2.3536]],\n",
      "\n",
      "         [[2.1548]],\n",
      "\n",
      "         [[2.5289]],\n",
      "\n",
      "         [[1.6702]],\n",
      "\n",
      "         [[2.4825]],\n",
      "\n",
      "         [[2.5229]],\n",
      "\n",
      "         [[1.8281]],\n",
      "\n",
      "         [[2.4911]],\n",
      "\n",
      "         [[1.4099]],\n",
      "\n",
      "         [[2.1475]],\n",
      "\n",
      "         [[1.9605]],\n",
      "\n",
      "         [[2.0139]],\n",
      "\n",
      "         [[2.3825]],\n",
      "\n",
      "         [[2.0534]],\n",
      "\n",
      "         [[2.2773]],\n",
      "\n",
      "         [[2.2360]],\n",
      "\n",
      "         [[1.8163]],\n",
      "\n",
      "         [[3.2700]],\n",
      "\n",
      "         [[2.1609]],\n",
      "\n",
      "         [[2.1287]],\n",
      "\n",
      "         [[1.8697]],\n",
      "\n",
      "         [[2.2451]],\n",
      "\n",
      "         [[3.4375]],\n",
      "\n",
      "         [[2.4354]],\n",
      "\n",
      "         [[2.9969]],\n",
      "\n",
      "         [[1.8035]],\n",
      "\n",
      "         [[1.6553]],\n",
      "\n",
      "         [[2.1847]],\n",
      "\n",
      "         [[2.3394]],\n",
      "\n",
      "         [[2.1477]],\n",
      "\n",
      "         [[1.9463]],\n",
      "\n",
      "         [[1.8643]],\n",
      "\n",
      "         [[2.5877]],\n",
      "\n",
      "         [[2.0660]],\n",
      "\n",
      "         [[2.0635]],\n",
      "\n",
      "         [[2.1967]],\n",
      "\n",
      "         [[2.6907]],\n",
      "\n",
      "         [[1.7282]],\n",
      "\n",
      "         [[2.2861]],\n",
      "\n",
      "         [[2.6865]],\n",
      "\n",
      "         [[2.2589]],\n",
      "\n",
      "         [[3.4438]],\n",
      "\n",
      "         [[2.4468]],\n",
      "\n",
      "         [[1.5867]],\n",
      "\n",
      "         [[2.2329]],\n",
      "\n",
      "         [[2.8378]],\n",
      "\n",
      "         [[2.3415]],\n",
      "\n",
      "         [[1.8542]],\n",
      "\n",
      "         [[1.8137]],\n",
      "\n",
      "         [[2.0884]],\n",
      "\n",
      "         [[2.3071]],\n",
      "\n",
      "         [[2.3047]],\n",
      "\n",
      "         [[2.4575]],\n",
      "\n",
      "         [[2.3583]],\n",
      "\n",
      "         [[2.2540]],\n",
      "\n",
      "         [[2.7175]],\n",
      "\n",
      "         [[2.7900]],\n",
      "\n",
      "         [[1.8971]],\n",
      "\n",
      "         [[2.4646]],\n",
      "\n",
      "         [[2.8995]],\n",
      "\n",
      "         [[2.5572]],\n",
      "\n",
      "         [[2.3475]],\n",
      "\n",
      "         [[2.0175]],\n",
      "\n",
      "         [[2.0423]],\n",
      "\n",
      "         [[1.8936]],\n",
      "\n",
      "         [[2.0116]],\n",
      "\n",
      "         [[1.6834]],\n",
      "\n",
      "         [[1.7938]],\n",
      "\n",
      "         [[2.4198]],\n",
      "\n",
      "         [[2.9654]],\n",
      "\n",
      "         [[1.6102]],\n",
      "\n",
      "         [[2.8806]],\n",
      "\n",
      "         [[1.6777]],\n",
      "\n",
      "         [[2.3231]],\n",
      "\n",
      "         [[2.1519]],\n",
      "\n",
      "         [[1.4586]],\n",
      "\n",
      "         [[2.8687]],\n",
      "\n",
      "         [[2.9338]],\n",
      "\n",
      "         [[1.9925]],\n",
      "\n",
      "         [[2.7323]],\n",
      "\n",
      "         [[2.9892]],\n",
      "\n",
      "         [[2.0119]],\n",
      "\n",
      "         [[2.5778]],\n",
      "\n",
      "         [[3.8697]],\n",
      "\n",
      "         [[2.4554]],\n",
      "\n",
      "         [[2.4460]],\n",
      "\n",
      "         [[2.3154]],\n",
      "\n",
      "         [[2.9038]],\n",
      "\n",
      "         [[2.8611]],\n",
      "\n",
      "         [[1.4923]],\n",
      "\n",
      "         [[2.2860]],\n",
      "\n",
      "         [[2.3473]],\n",
      "\n",
      "         [[2.2460]],\n",
      "\n",
      "         [[1.7970]],\n",
      "\n",
      "         [[2.2436]],\n",
      "\n",
      "         [[2.3331]],\n",
      "\n",
      "         [[2.0469]],\n",
      "\n",
      "         [[2.3106]],\n",
      "\n",
      "         [[2.2609]],\n",
      "\n",
      "         [[2.1394]],\n",
      "\n",
      "         [[2.1271]],\n",
      "\n",
      "         [[2.1650]],\n",
      "\n",
      "         [[2.6583]],\n",
      "\n",
      "         [[2.3923]],\n",
      "\n",
      "         [[2.4442]],\n",
      "\n",
      "         [[1.8400]],\n",
      "\n",
      "         [[2.9909]],\n",
      "\n",
      "         [[2.3664]],\n",
      "\n",
      "         [[1.8878]],\n",
      "\n",
      "         [[2.2650]],\n",
      "\n",
      "         [[2.0667]],\n",
      "\n",
      "         [[2.6562]],\n",
      "\n",
      "         [[1.9384]],\n",
      "\n",
      "         [[2.0705]],\n",
      "\n",
      "         [[2.6156]],\n",
      "\n",
      "         [[2.0737]],\n",
      "\n",
      "         [[1.3865]],\n",
      "\n",
      "         [[2.5060]],\n",
      "\n",
      "         [[3.6062]],\n",
      "\n",
      "         [[1.6854]],\n",
      "\n",
      "         [[1.7345]],\n",
      "\n",
      "         [[1.5582]],\n",
      "\n",
      "         [[1.6535]],\n",
      "\n",
      "         [[1.9800]],\n",
      "\n",
      "         [[1.8299]],\n",
      "\n",
      "         [[2.0340]],\n",
      "\n",
      "         [[2.9693]],\n",
      "\n",
      "         [[2.6916]],\n",
      "\n",
      "         [[2.7121]],\n",
      "\n",
      "         [[1.8925]],\n",
      "\n",
      "         [[3.2354]],\n",
      "\n",
      "         [[2.6444]],\n",
      "\n",
      "         [[2.6152]],\n",
      "\n",
      "         [[2.4198]],\n",
      "\n",
      "         [[2.2983]],\n",
      "\n",
      "         [[2.1516]],\n",
      "\n",
      "         [[1.9476]],\n",
      "\n",
      "         [[1.9496]],\n",
      "\n",
      "         [[1.6945]],\n",
      "\n",
      "         [[2.0827]],\n",
      "\n",
      "         [[1.9940]],\n",
      "\n",
      "         [[2.2815]],\n",
      "\n",
      "         [[2.4014]],\n",
      "\n",
      "         [[2.1235]],\n",
      "\n",
      "         [[2.8366]],\n",
      "\n",
      "         [[2.2643]],\n",
      "\n",
      "         [[1.6311]],\n",
      "\n",
      "         [[1.9258]],\n",
      "\n",
      "         [[1.3314]],\n",
      "\n",
      "         [[1.7159]]]], grad_fn=<MaxPool2DWithIndicesBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransitionLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(TransitionLayer, self).__init__()\n",
    "        \n",
    "        # Convolutional layer with kernel size 1x1\n",
    "        self.conv1x1 = nn.Conv2d(input_channels, output_channels, kernel_size=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm2d(output_channels)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling with kernel size equal to the feature map size\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply operations sequentially\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "input_channels = 512\n",
    "output_channels = 300 # D is the desired number of channels\n",
    "transition_layer = TransitionLayer(input_channels, output_channels)\n",
    "\n",
    "# Assuming input_tensor is of size (batch_size, 512, 7, 7)\n",
    "input_tensor = torch.randn(3, 512, 7, 7)\n",
    "\n",
    "# Apply the transition layer\n",
    "output = transition_layer(input_tensor)\n",
    "\n",
    "# Print the shape of the final feature vector\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIIIIG. We know the feature space at the end would be 512, 7, 7. \n",
    "This means that, if we want to create an encoder that would take this; and convert it to a sampling gaussian representation <br>\n",
    "we need to do the following:\n",
    "1. Pass a VGGnet16 pretrained features at eval mode (option to pretrain it too) to 512, 7, 7.\n",
    "2. Pass that VGGnet16 through a transition layer that would flat it to a num_channelsx 300 output (3 since RGB).\n",
    "3. Use this to sample a mean and gaussian distribution uing homework codes to retrieve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function that would sample gaussian parameters.\n",
    "Use functions from hw2 utils.py to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "def gaussian_parameters(h, dim=-1):\n",
    "    \"\"\"\n",
    "    Converts generic real-valued representations into mean and variance\n",
    "    parameters of a Gaussian distribution\n",
    "\n",
    "    Args:\n",
    "        h: tensor: (batch, ..., dim, ...): Arbitrary tensor\n",
    "        dim: int: (): Dimension along which to split the tensor for mean and\n",
    "            variance\n",
    "\n",
    "    Returns:\n",
    "        m: tensor: (batch, ..., dim / 2, ...): Mean\n",
    "        v: tensor: (batch, ..., dim / 2, ...): Variance\n",
    "    \"\"\"\n",
    "    print(f\"h dimension passed through gaussian paremeters i {h.shape}\")\n",
    "    m, h = torch.split(h, h.size(dim) // 2, dim=dim)\n",
    "    v = F.softplus(h) + 1e-8\n",
    "    return m, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Encoder\n",
    "Now that we have replcated all of the transformations required to perform the encoder first time .<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of a single image and its labels\n",
      "Image: torch.Size([3, 224, 224]), labels: torch.Size([20])\n",
      "batch number: 0\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 1\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 2\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 3\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 4\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "batch number: 5\n",
      "Shape of batch of images and labels\n",
      "Images: torch.Size([16, 3, 224, 224]), labels: torch.Size([16, 20])\n",
      "h dimension passed through gaussian paremeters i torch.Size([16, 4])\n",
      "It can iterate through all batches\n",
      "torch.Size([16, 2])\n",
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self,  z_dim, y_dim=0, pretrained=True,):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.y_dim = y_dim\n",
    "        # Load pre-trained VGG16 model\n",
    "        vgg16_model = models.vgg16(weights=pretrained)\n",
    "\n",
    "        # Use only the features part and remove the classifier\n",
    "        self.features = vgg16_model.features\n",
    "\n",
    "        # Set to evaluation mode if not fine-tuning\n",
    "        if not pretrained:\n",
    "            self.features.eval()\n",
    "        \n",
    "        # Convolutional layer with kernel size 1x1\n",
    "        self.conv1x1 = nn.Conv2d(512, 300, kernel_size=1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm2d(300)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Max pooling with kernel size equal to the feature map size\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=7)\n",
    "\n",
    "        #Obtain the net\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(300, 2 * z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Create feature map from vgget16\n",
    "        feat_map = self.features(x)\n",
    "\n",
    "        # Apply operations to obtain transition layer from paper\n",
    "        h = self.conv1x1(feat_map)\n",
    "        h = self.batch_norm(h)\n",
    "        h = self.relu(h)\n",
    "        h = self.max_pool(h)\n",
    "\n",
    "        # Convert output from 3, 300, 1, 1 to 3, 300\n",
    "        h = h.view(h.shape[0], h.shape[1])\n",
    "\n",
    "        #Now pass it through the net to obtain gaussian space\n",
    "        g = self.net(h)\n",
    "\n",
    "        #Pass the feature space and get gaussian parameters\n",
    "        m, v = gaussian_parameters(g, dim=1)\n",
    "        return m, v\n",
    "    \n",
    "#Sampled images from train to see single shape\n",
    "samp3_image, label3 = train_dataset[1]\n",
    "print(\"Shape of a single image and its labels\")\n",
    "print(f\"Image: {samp3_image.shape}, labels: {label3.shape}\")\n",
    "\n",
    "#Create encoder compiler\n",
    "encoder_compiler = Encoder(z_dim = 2)\n",
    "\n",
    "#Iterate inside a batch\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    print(f\"batch number: {idx}\")\n",
    "    images, labels = batch\n",
    "    print(\"Shape of batch of images and labels\")\n",
    "    print(f\"Images: {images.shape}, labels: {labels.shape}\")\n",
    "    output = encoder_compiler(images)\n",
    "    if idx == 5:\n",
    "        print(\"It can iterate through all batches\")\n",
    "        break\n",
    "\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
